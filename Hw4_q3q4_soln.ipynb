{
  "cells": [
    {
      "cell_type": "markdown",
      "source": "# Hw4-Written Hw Q3 and 4",
      "metadata": {
        "id": "h4OLm1o5m7aw",
        "cell_id": "00000-a47a1144-3074-4893-8dde-02d6f694faeb",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "76J_-Ojem7a2",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "7e97df74",
        "execution_millis": 943,
        "cell_id": "00002-6a72fd2d-59ca-4747-a08c-32a444c5b747",
        "execution_start": 1616699763525,
        "deepnote_cell_type": "code"
      },
      "source": "# Import Important Libraries\nimport sklearn\nfrom sklearn.datasets import load_breast_cancer # taking included data set from Sklearn http://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_breast_cancer.html\nfrom sklearn.linear_model import LogisticRegression # importing Sklearn's logistic regression's module\nfrom sklearn import preprocessing # preprossing is what we do with the data before we run the learning algorithm\nfrom sklearn.model_selection import train_test_split \nimport numpy as np\n\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\n%matplotlib inline",
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "HW4, q3 ",
      "metadata": {
        "tags": [],
        "cell_id": "00003-69224833-c34f-45bb-a438-df0754ec5cd8",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "code",
      "source": "import numpy as np\n\ndef hypothesis(x , w):\n    z = np.dot(x, w)\n    return 1/(1 + np.exp(-z))\n\ndef likelihood(x, y, w):\n    yhat = hypothesis(X, w)\n    likelihood = np.sum(y * np.log(yhat) + (1 - y) * np.log(1 - yhat)) \n    return likelihood\n\nX = np.array([[0.49,0.09,1.69,0.04,0.04, 0.64, 1, 0.16],[0.16,0.09,0.25,0,0.49,0,0.04,0.01]]).reshape(8,2)\nx_one = np.ones((8,1)) # add one column\nX = np.hstack((x_one, X))\n\nw_1 = np.array([0.66,-2.24,-0.18]).reshape(3,1)\nw_2 = np.array([1.33, -2.96, -2.77]).reshape(3,1)\n\ny = np.array([0,0,0,0,1,1,1,1]).reshape(8,1)\n\nprint(f'Check X =\\n {X}, the shape of X is {X.shape}')\nprint(f'Check y = {y.transpose()}, the shape of X is {y.shape}')\nprint(f'Check w = {w_1.transpose()}, the shape of w is {w_1.shape}')\nprint(f'Check w\\' = {w_2.transpose()}, the shape of w\\' is {w_2.shape}')\n\nprint(f'h_w(x) is:\\n {hypothesis(X, w_1)}')\nprint(f'h_w\\'(x) is:\\n {hypothesis(X, w_2)}')\n\nprint(f'likelihood of w is {likelihood(X,y,w_1)}')\nprint(f'likelihood of w\\' is {likelihood(X,y,w_2)}')\n",
      "metadata": {
        "tags": [],
        "cell_id": "00003-9ef97e5a-6acc-4728-813b-e5780e53812e",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "167f7e18",
        "execution_start": 1616699764480,
        "execution_millis": 27,
        "deepnote_cell_type": "code"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Check X =\n [[1.   0.49 0.09]\n [1.   1.69 0.04]\n [1.   0.04 0.64]\n [1.   1.   0.16]\n [1.   0.16 0.09]\n [1.   0.25 0.  ]\n [1.   0.49 0.  ]\n [1.   0.04 0.01]], the shape of X is (8, 3)\nCheck y = [[0 0 0 0 1 1 1 1]], the shape of X is (8, 1)\nCheck w = [[ 0.66 -2.24 -0.18]], the shape of w is (3, 1)\nCheck w' = [[ 1.33 -2.96 -2.77]], the shape of w' is (3, 1)\nh_w(x) is:\n [[0.38845766]\n [0.04177438]\n [0.61187487]\n [0.16675528]\n [0.57086961]\n [0.52497919]\n [0.39231299]\n [0.63844007]]\nh_w'(x) is:\n [[0.40861351]\n [0.02224374]\n [0.36326985]\n [0.11172906]\n [0.64727899]\n [0.64336515]\n [0.46993631]\n [0.76564831]]\nlikelihood of w is -4.25271239754217\nlikelihood of w' is -3.015879343821497\n",
          "output_type": "stream"
        }
      ],
      "execution_count": 2
    },
    {
      "cell_type": "markdown",
      "source": "# Loading the data set.\n\nIn the below code cell, you will load the data from sklearn using the method given. Check import statements and use the given function",
      "metadata": {
        "id": "xeiMRQlFm7a3",
        "cell_id": "00003-231347f9-653f-47fa-a740-fa9ee5d04be1",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lpnLacVvm7a3",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "40bc4489",
        "execution_millis": 13,
        "cell_id": "00004-b73b62a4-cd26-4133-bbf1-68f53ef4e98f",
        "execution_start": 1616699764512,
        "deepnote_cell_type": "code"
      },
      "source": "# TODO Q01\ncancer = load_breast_cancer()",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "becyu2dPm7a3",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1615639173675,
          "user_tz": 300,
          "elapsed": 201,
          "user": {
            "displayName": "Kevin Chen",
            "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GheacSJEHNzOW-i0Y1O_QQhtN5-kcSRe1oDSccbFw=s64",
            "userId": "05601742559097284347"
          }
        },
        "outputId": "5527bb55-dab5-4f3a-b3b4-b48ea90f5dd2",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "110de928",
        "execution_millis": 7,
        "cell_id": "00006-6130dd91-040b-4626-819d-47f5eb5133f9",
        "execution_start": 1616699764531,
        "deepnote_cell_type": "code"
      },
      "source": "# VERIFY - Print the shape of data and target\nprint('Q01 - cancer.target.shape: ', cancer.target.shape)\nprint('Q01 - cancer.data.shape: ', cancer.data.shape)",
      "execution_count": 4,
      "outputs": [
        {
          "name": "stdout",
          "text": "Q01 - cancer.target.shape:  (569,)\nQ01 - cancer.data.shape:  (569, 30)\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": "# Data Pre-Processing\nScale before splitting the data into train and test since we will be using gradient ascent. \n* Use `preprocessing` to scale the data. \n* Assign the target of cancer to variable `y (<np.ndarray>)`.\n* Use `train_test_split` to split the data (`75% train` and `25% test`) to `X_train`, `X_test`, `y_train`, `y_test` with `random_state` of 42\n* Reshape `y_train` into 2D array `y_2d_train` and `y_test` into 2D array `y_2d_test`",
      "metadata": {
        "id": "qp9GnJDTm7a4",
        "cell_id": "00007-d51d3516-3527-4cb8-a272-d12926c1fc8e",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u6nCenfLm7a4",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "7ee83332",
        "execution_millis": 0,
        "cell_id": "00008-f4fc8ac0-94b8-47cf-9ab6-97edb00231e0",
        "execution_start": 1616699764567,
        "deepnote_cell_type": "code"
      },
      "source": "# TODO Q02\nX_scale = preprocessing.scale(cancer.data)\ny = cancer.target\n\nX_train, X_test, y_train, y_test = train_test_split(X_scale, y, random_state=42, train_size=0.75, test_size=0.25)\n# print(X_train)\n# print(X_test)\n# print(y_train)\n# print(y_test)\ny_2d_train = y_train.reshape((y_train.shape[0], 1))\ny_2d_test = y_test.reshape((y_test.shape[0], 1))\n# print(y_2d_train)\n# print(y_2d_test)\n",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "# Implementing Logistic Regression Using Gradient Ascent",
      "metadata": {
        "id": "EaHeQwYcm7a5",
        "cell_id": "00012-ddf1363a-09fc-410c-ad53-34a643a17009",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "OFZ106QHm7a5",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "a043e4b4",
        "execution_millis": 0,
        "cell_id": "00013-92ab9180-e560-4d45-ab96-63f1d09be1c6",
        "execution_start": 1616699764568,
        "deepnote_cell_type": "code"
      },
      "source": "# TODO Q03\n# Write the sigmoid function\ndef sigmoid(z):\n    result = 1 / (1 + np.exp(-z))\n    return result",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kK3Mmsahm7a6",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "4b38b765",
        "execution_millis": 0,
        "cell_id": "00016-3d81c847-8891-4ca2-8cb9-863035e6b598",
        "execution_start": 1616699764569,
        "deepnote_cell_type": "code"
      },
      "source": "# TODO Q04\n# Append a column of ones to X_train\n# ones is a  vector of shape n,1\nones = np.ones((X_train.shape[0], 1))\n# print(ones)\n# Append a column of ones in the beginning of X_train an save in variable X_train_1(<np.ndarray>).\nX_train_1 = np.hstack((ones, X_train))",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tZQ-fckjm7a7",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "b5182002",
        "execution_millis": 3,
        "cell_id": "00019-49867edb-ab4b-4c58-83c8-54c694d57913",
        "execution_start": 1616699764570,
        "deepnote_cell_type": "code"
      },
      "source": "# TODO Q05\n# Initialize Parameter Vector w to a zero matrix with shape (X_train_1.shape[1],1)\nw_init = np.zeros(((X_train_1.shape[1],1)))",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H7ljMAiHm7a7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "status": "ok",
          "timestamp": 1615641121233,
          "user_tz": 300,
          "elapsed": 144,
          "user": {
            "displayName": "Kevin Chen",
            "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GheacSJEHNzOW-i0Y1O_QQhtN5-kcSRe1oDSccbFw=s64",
            "userId": "05601742559097284347"
          }
        },
        "outputId": "254e60d1-933b-46d8-94b5-5f9be15f8184",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "dc9b7a50",
        "execution_millis": 35,
        "cell_id": "00021-6ffe0e7c-60d2-4a9c-bce6-2c28c506b764",
        "execution_start": 1616699764574,
        "deepnote_cell_type": "code"
      },
      "source": "# VERIFY\nprint('Q05 - w_init.shape: ', w_init.shape)",
      "execution_count": 9,
      "outputs": [
        {
          "name": "stdout",
          "text": "Q05 - w_init.shape:  (31, 1)\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "70yYZoVQm7a7",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "7901ba38",
        "execution_millis": 1,
        "cell_id": "00022-0c700a73-a3c0-4103-95bd-a87030b60b39",
        "execution_start": 1616699764586,
        "deepnote_cell_type": "code"
      },
      "source": "# TODO Q06\n# Write the hypothesis function\n# this returns a vector of all sigmoids of all examples\n# in other words, returns the probability between 0 and 1 of being class 1 of each example \ndef hypothesis(X_train_1, w):\n    z_score = X_train_1.dot(w)\n    y_hat = sigmoid(z_score)\n    return y_hat",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LOKbZwM2m7a7",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1616181768387,
          "user_tz": 240,
          "elapsed": 227,
          "user": {
            "displayName": "Kevin Chen",
            "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GheacSJEHNzOW-i0Y1O_QQhtN5-kcSRe1oDSccbFw=s64",
            "userId": "05601742559097284347"
          }
        },
        "deepnote_to_be_reexecuted": false,
        "source_hash": "5f0c6440",
        "execution_millis": 8,
        "cell_id": "00024-cbda64fd-7637-4bc0-b194-3e8a193869e0",
        "execution_start": 1616699764588,
        "deepnote_cell_type": "code"
      },
      "source": "# TODO Q07 \n# Compute y_hat(<np.ndarray>) using X_train_1 and w_init\ny_hat_init = hypothesis(X_train_1, w_init)",
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "# Likelihood Function.\nWrite the code to calculate the log likelihood as discussed in the class.",
      "metadata": {
        "id": "BMFyz4Itm7a8",
        "cell_id": "00027-27bfbfa5-a4a5-4410-b11e-e513af1a86d2",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "gMOZEDT7m7a8",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "40533a7c",
        "execution_millis": 13,
        "cell_id": "00028-19bd9154-bd76-4b2f-b5c6-32438d2cd6d2",
        "execution_start": 1616699764599,
        "deepnote_cell_type": "code"
      },
      "source": "# TODO Q08\n# Write the log likelihood function\ndef likelihood(X_tr, y_tr, w, n):\n    y_hat = hypothesis(X_tr, w) # is a vector of all probability of each example, basically h(x^(i)) for all i = 0 .. n\n    likelihood = np.sum(y_tr * np.log(y_hat) + (1 - y_tr) * np.log(1 - y_hat))\n    return likelihood",
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ERKwBCsum7a8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "status": "ok",
          "timestamp": 1615646542420,
          "user_tz": 300,
          "elapsed": 200,
          "user": {
            "displayName": "Kevin Chen",
            "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GheacSJEHNzOW-i0Y1O_QQhtN5-kcSRe1oDSccbFw=s64",
            "userId": "05601742559097284347"
          }
        },
        "outputId": "bff8aeab-e4d6-411f-d2ef-fc0900cc40f0",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "e2e61a3c",
        "execution_millis": 52,
        "cell_id": "00030-fe9140fb-457f-4dcf-ab49-bafa247ee3b8",
        "execution_start": 1616699764620,
        "deepnote_cell_type": "code"
      },
      "source": "# VERIFY - The value should be equal to -295.2806989185367 using X_train_1, y_2d_train, w, X_train_1.shape[0].\nprint('Q08 - likelihood: ', likelihood(X_train_1, y_2d_train, w_init, X_train_1.shape[0]))",
      "execution_count": 13,
      "outputs": [
        {
          "name": "stdout",
          "text": "Q08 - likelihood:  -295.2806989185367\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": "# Gradient Ascent with Ridge Regression",
      "metadata": {
        "id": "3_eZ-Iomm7a8",
        "cell_id": "00031-72e42785-56d7-4995-9b95-9b01855e5825",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "gi1oB0gFm7a9",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "a7f2ee4f",
        "execution_millis": 5,
        "cell_id": "00032-4c0ff72b-6483-4edf-9f63-5e0ed43ad7db",
        "execution_start": 1616699764646,
        "deepnote_cell_type": "code"
      },
      "source": "# TODO Q09\n# Write the gradient ascent function\ndef Gradient_Ascent(X_train_1, y_2d_train, learning_rate, num_iters, alpha):\n    # Number of training examples.\n    N = X_train_1.shape[0]\n    # Initialize w(<np.ndarray>). Zeros vector of shape X_train.shape[1],1\n    w = np.zeros((X_train_1.shape[1], 1))\n    # Initiating list to store values of likelihood(<list>) after few iterations.\n    likelihood_values = []\n    for i in range(num_iters):\n        y_hat = hypothesis(X_train_1, w) \n        error = y_2d_train - y_hat \n        gradient = X_train_1.T.dot(error) \n        # Updating Parameters\n        ridgeDeriv = 2 * w \n        w = w + (learning_rate / N) * gradient - alpha *  ridgeDeriv\n        if (i % 100) == 0:\n            likelihood_values.append(likelihood(X_train_1,y_2d_train,w,N))\n        \n    return w, likelihood_values",
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "deepnote_to_be_reexecuted": false,
        "source_hash": "5ac0903f",
        "execution_millis": 719,
        "cell_id": "00033-a9d8335d-8753-473f-82bb-e817702ce31f",
        "execution_start": 1616699764652,
        "deepnote_cell_type": "code"
      },
      "source": "learning_rate = 0.75\nnum_iters = 10000\nalpha = 0.0001 #lambda in this case\nw_gradient_ascent, likelihood_values = Gradient_Ascent(X_train_1, y_2d_train, learning_rate, num_iters, alpha)\nprint(w_gradient_ascent)\n",
      "execution_count": 15,
      "outputs": [
        {
          "name": "stdout",
          "text": "[[-0.3371506 ]\n [-0.07965755]\n [-0.16694649]\n [ 0.11512502]\n [-0.31074216]\n [-0.37634795]\n [ 2.60738557]\n [-1.27817525]\n [-3.02236987]\n [ 0.88708933]\n [-0.84121432]\n [-3.24493654]\n [ 0.65548693]\n [-0.17323638]\n [-2.11810957]\n [-0.31472541]\n [ 0.24429312]\n [ 0.18858105]\n [-1.26221572]\n [ 1.30240782]\n [ 1.92300053]\n [-1.56868421]\n [-2.44457231]\n [-0.06114289]\n [-1.70997665]\n [-0.15616728]\n [ 0.64091936]\n [-2.23502552]\n [-0.80024716]\n [-2.56680736]\n [-0.4002247 ]]\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "cell_id": "00036-edb59ab9-9670-4d0f-ab88-f1f4a6870d13",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "94b78632",
        "execution_millis": 13,
        "execution_start": 1616699765380,
        "deepnote_cell_type": "code"
      },
      "source": "def predict(X_train_1, w):\n    y_hat = sigmoid(X_train_1.dot(w))\n    return y_hat",
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "deepnote_to_be_reexecuted": false,
        "source_hash": "d8c3d0e6",
        "execution_millis": 18,
        "cell_id": "00037-de3dcdf2-7f56-4fa3-9037-703ff9d83a7d",
        "execution_start": 1616699765397,
        "deepnote_cell_type": "code"
      },
      "source": "from sklearn.model_selection import KFold\n\ndef kfold_cross_validation(X_train_1, y):\n    lambdas = [1,0.00001,0.001, 0.1, 10, 0.01, 0.0001, 0.00000001]\n    #print(\"All possible lambdas:\", lambdas)\n    best_lambda = 0\n    kfold_model = KFold(n_splits=5, random_state=None, shuffle=False)\n    min_error = float('inf')\n    w_best = []\n    for l in lambdas:\n        #print(l)\n        error = []\n        wval = []\n        for train_index, test_index in kfold_model.split(X_train_1):\n            X_tr = X_train_1[train_index]\n            Y_tr = y[train_index].reshape((-1, 1))\n            X_ts = X_train_1[test_index]\n            Y_ts = y[test_index].reshape((-1, 1))\n            w, likelihood = Gradient_Ascent(X_tr, Y_tr, 0.001, X_tr.shape[0], l)\n            yhat = hypothesis(X_ts, w)\n            error.append(np.sum((y-yhat)**2))\n            wval.append(w)\n\n        if (np.mean(error) < min_error):\n            min_error = np.mean(error)\n            w_best = wval[np.argmin(error)]\n            best_lambda = l\n\n        #print(\"Min error\",min_error)\n            \n    return w_best, best_lambda",
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "deepnote_to_be_reexecuted": false,
        "source_hash": "85a83b6b",
        "execution_millis": 986,
        "cell_id": "00038-d2762beb-3392-4f36-a239-7481110f7e3f",
        "execution_start": 1616699765421,
        "deepnote_cell_type": "code"
      },
      "source": "w_best, best_lambda = kfold_cross_validation(X_train_1, y)\nprint(\"Best lambda:\", best_lambda)",
      "execution_count": 18,
      "outputs": [
        {
          "name": "stderr",
          "text": "/shared-libs/python3.7/py-core/lib/python3.7/site-packages/ipykernel_launcher.py:4: RuntimeWarning: overflow encountered in exp\n  after removing the cwd from sys.path.\n/shared-libs/python3.7/py-core/lib/python3.7/site-packages/ipykernel_launcher.py:5: RuntimeWarning: divide by zero encountered in log\n  \"\"\"\n/shared-libs/python3.7/py-core/lib/python3.7/site-packages/ipykernel_launcher.py:5: RuntimeWarning: invalid value encountered in multiply\n  \"\"\"\n/shared-libs/python3.7/py-core/lib/python3.7/site-packages/ipykernel_launcher.py:16: RuntimeWarning: overflow encountered in multiply\n  app.launch_new_instance()\n/shared-libs/python3.7/py-core/lib/python3.7/site-packages/ipykernel_launcher.py:17: RuntimeWarning: overflow encountered in multiply\nBest lambda: 1e-08\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=76220f38-ae8c-4d04-96a1-7ad7bcfb080e' target=\"_blank\">\n<img style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,PD94bWwgdmVyc2lvbj0iMS4wIiBlbmNvZGluZz0iVVRGLTgiPz4KPHN2ZyB3aWR0aD0iODBweCIgaGVpZ2h0PSI4MHB4IiB2aWV3Qm94PSIwIDAgODAgODAiIHZlcnNpb249IjEuMSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIiB4bWxuczp4bGluaz0iaHR0cDovL3d3dy53My5vcmcvMTk5OS94bGluayI+CiAgICA8IS0tIEdlbmVyYXRvcjogU2tldGNoIDU0LjEgKDc2NDkwKSAtIGh0dHBzOi8vc2tldGNoYXBwLmNvbSAtLT4KICAgIDx0aXRsZT5Hcm91cCAzPC90aXRsZT4KICAgIDxkZXNjPkNyZWF0ZWQgd2l0aCBTa2V0Y2guPC9kZXNjPgogICAgPGcgaWQ9IkxhbmRpbmciIHN0cm9rZT0ibm9uZSIgc3Ryb2tlLXdpZHRoPSIxIiBmaWxsPSJub25lIiBmaWxsLXJ1bGU9ImV2ZW5vZGQiPgogICAgICAgIDxnIGlkPSJBcnRib2FyZCIgdHJhbnNmb3JtPSJ0cmFuc2xhdGUoLTEyMzUuMDAwMDAwLCAtNzkuMDAwMDAwKSI+CiAgICAgICAgICAgIDxnIGlkPSJHcm91cC0zIiB0cmFuc2Zvcm09InRyYW5zbGF0ZSgxMjM1LjAwMDAwMCwgNzkuMDAwMDAwKSI+CiAgICAgICAgICAgICAgICA8cG9seWdvbiBpZD0iUGF0aC0yMCIgZmlsbD0iIzAyNjVCNCIgcG9pbnRzPSIyLjM3NjIzNzYyIDgwIDM4LjA0NzY2NjcgODAgNTcuODIxNzgyMiA3My44MDU3NTkyIDU3LjgyMTc4MjIgMzIuNzU5MjczOSAzOS4xNDAyMjc4IDMxLjY4MzE2ODMiPjwvcG9seWdvbj4KICAgICAgICAgICAgICAgIDxwYXRoIGQ9Ik0zNS4wMDc3MTgsODAgQzQyLjkwNjIwMDcsNzYuNDU0OTM1OCA0Ny41NjQ5MTY3LDcxLjU0MjI2NzEgNDguOTgzODY2LDY1LjI2MTk5MzkgQzUxLjExMjI4OTksNTUuODQxNTg0MiA0MS42NzcxNzk1LDQ5LjIxMjIyODQgMjUuNjIzOTg0Niw0OS4yMTIyMjg0IEMyNS40ODQ5Mjg5LDQ5LjEyNjg0NDggMjkuODI2MTI5Niw0My4yODM4MjQ4IDM4LjY0NzU4NjksMzEuNjgzMTY4MyBMNzIuODcxMjg3MSwzMi41NTQ0MjUgTDY1LjI4MDk3Myw2Ny42NzYzNDIxIEw1MS4xMTIyODk5LDc3LjM3NjE0NCBMMzUuMDA3NzE4LDgwIFoiIGlkPSJQYXRoLTIyIiBmaWxsPSIjMDAyODY4Ij48L3BhdGg+CiAgICAgICAgICAgICAgICA8cGF0aCBkPSJNMCwzNy43MzA0NDA1IEwyNy4xMTQ1MzcsMC4yNTcxMTE0MzYgQzYyLjM3MTUxMjMsLTEuOTkwNzE3MDEgODAsMTAuNTAwMzkyNyA4MCwzNy43MzA0NDA1IEM4MCw2NC45NjA0ODgyIDY0Ljc3NjUwMzgsNzkuMDUwMzQxNCAzNC4zMjk1MTEzLDgwIEM0Ny4wNTUzNDg5LDc3LjU2NzA4MDggNTMuNDE4MjY3Nyw3MC4zMTM2MTAzIDUzLjQxODI2NzcsNTguMjM5NTg4NSBDNTMuNDE4MjY3Nyw0MC4xMjg1NTU3IDM2LjMwMzk1NDQsMzcuNzMwNDQwNSAyNS4yMjc0MTcsMzcuNzMwNDQwNSBDMTcuODQzMDU4NiwzNy43MzA0NDA1IDkuNDMzOTE5NjYsMzcuNzMwNDQwNSAwLDM3LjczMDQ0MDUgWiIgaWQ9IlBhdGgtMTkiIGZpbGw9IiMzNzkzRUYiPjwvcGF0aD4KICAgICAgICAgICAgPC9nPgogICAgICAgIDwvZz4KICAgIDwvZz4KPC9zdmc+' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>",
      "metadata": {
        "tags": [],
        "created_in_deepnote_cell": true,
        "deepnote_cell_type": "markdown"
      }
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.2-final"
    },
    "colab": {
      "name": "HW4_prog_kc3585.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "iQsyXlPIm7bA",
        "RYQ6RpMxm7bC"
      ]
    },
    "deepnote_notebook_id": "95be0398-c861-4d68-b4b9-f966f3109332",
    "deepnote": {},
    "deepnote_execution_queue": []
  }
}