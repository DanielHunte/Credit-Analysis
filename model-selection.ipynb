{
 "metadata": {
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python394jvsc74a57bd0daf19b0280700678d1b30283c052a0b4f397f6934ace6c653c7a54482fc82c11",
   "display_name": "Python 3.9.4 64-bit ('venv')"
  },
  "metadata": {
   "interpreter": {
    "hash": "9518128f597d7b00dc14729602cfd87fb7b2cf75925976bcb0d0e328a830a12b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Imports"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "source": [
    "# Data Cleaning"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Selecting Relevant Fields\n",
    "the dataset will be loaded and transformed and relevant dimensions will remain"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('database.csv')[[\n",
    "    'NAME_CONTRACT_STATUS',\n",
    "    'CODE_GENDER',\n",
    "    'FLAG_OWN_CAR',\n",
    "    'FLAG_OWN_REALTY',\n",
    "    'AMT_INCOME_TOTAL',\n",
    "    'AMT_CREDIT',\n",
    "    'NAME_INCOME_TYPE',\n",
    "    'NAME_EDUCATION_TYPE',\n",
    "    'NAME_FAMILY_STATUS',\n",
    "    'NAME_HOUSING_TYPE',\n",
    "    'DAYS_BIRTH',\n",
    "    'DAYS_EMPLOYED',\n",
    "    'OCCUPATION_TYPE',\n",
    "    'CNT_FAM_MEMBERS',\n",
    "    'NAME_CLIENT_TYPE',\n",
    "    'AMT_DOWN_PAYMENT',\n",
    "    'NAME_TYPE_SUITE',\n",
    "    'NAME_PAYMENT_TYPE'\n",
    "]]\n",
    "\n",
    "#drop rows with na values\n",
    "df = df.dropna()\n",
    "for col in df.columns:\n",
    "    df = df[df[col]!='XNA'].reset_index(drop=True)\n",
    "\n",
    "#shuffle the dataframe\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "df.head(6)"
   ]
  },
  {
   "source": [
    "## Reductions to Binary Variables\n",
    "\n",
    "The target variable, NAME_CONTRACT_STATUS, will be reduced from one of 4 possible values, to one of two generic but still correct values – for example, the dataset distingushes between cancelled and rejected and granted loans, however we will only distinguish between granted and not granted loans. Values that are binary but that don't use the binary alphabet will be transformed to use the binary alphabet as well.\n",
    "\n",
    "Defining a function that will return a copy of the dataframe with reduced fields."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce(df, name, value):\n",
    "    if type(name) != str:\n",
    "        raise Exception('only one dimension is reduced at a time')\n",
    "    idx_name = df.columns.get_loc(name)\n",
    "    reduced = [(1 if df[name][i] == value else 0) for i in range(len(df))]\n",
    "    df_reduced = df.drop(labels=[name], axis=1)\n",
    "    df_reduced.insert(loc=idx_name, column=name, value=reduced)\n",
    "    return df_reduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reduced = df\n",
    "df_reduced = reduce(df_reduced, 'NAME_CONTRACT_STATUS', 'Approved') # 1 if approved else 0\n",
    "df_reduced = reduce(df_reduced, 'CODE_GENDER', 'M') # 1 if male else 0\n",
    "df_reduced = reduce(df_reduced, 'FLAG_OWN_CAR', 'Y') # 1 if owns car else 0\n",
    "df_reduced = reduce(df_reduced, 'FLAG_OWN_REALTY', 'Y') # 1 if owns property else 0\n",
    "\n",
    "df_reduced.head(10)"
   ]
  },
  {
   "source": [
    "## One-Hot Encoding\n",
    "defining a function that returns a copy of the input dataframe with a specific dimension one-hot encoded"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def one_hot_encode(df, name, prefix=''):\n",
    "    if type(name) != str:\n",
    "        raise Exception('one hot encoding applies to one dimension at a time')\n",
    "    if len(df) == 0:\n",
    "        raise Exception('dataframe is empty')\n",
    "    \n",
    "    df = df.copy()\n",
    "    values = df[name].unique()\n",
    "    \n",
    "    #for each unique value, we create a new column where df[row][new column] is 1 if the value of df[row][value] == new column\n",
    "    for v in values:\n",
    "        one_hot_column = [(1 if df[name][i] == v else 0) for i in range(len(df))]\n",
    "        df.insert(loc=len(df.loc[0]), column=prefix + str(v), value=one_hot_column)\n",
    "\n",
    "    return df.drop(labels=[name], axis=1)"
   ]
  },
  {
   "source": [
    "performing one-hot encoding on any dimension whose values are one of a set of string values"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encoded = df_reduced\n",
    "df_encoded = one_hot_encode(df_encoded, 'NAME_INCOME_TYPE')\n",
    "df_encoded = one_hot_encode(df_encoded, 'NAME_EDUCATION_TYPE')\n",
    "df_encoded = one_hot_encode(df_encoded, 'NAME_FAMILY_STATUS')\n",
    "df_encoded = one_hot_encode(df_encoded, 'NAME_HOUSING_TYPE')\n",
    "df_encoded = one_hot_encode(df_encoded, 'OCCUPATION_TYPE')\n",
    "df_encoded = one_hot_encode(df_encoded, 'CNT_FAM_MEMBERS', prefix='CNT_FAM_MEM_')\n",
    "df_encoded = one_hot_encode(df_encoded, 'NAME_CLIENT_TYPE')\n",
    "df_encoded = one_hot_encode(df_encoded, 'NAME_TYPE_SUITE')\n",
    "df_encoded = one_hot_encode(df_encoded, 'NAME_PAYMENT_TYPE')\n",
    "df_all_features = df_encoded\n",
    "\n",
    "#sanity check the dataframe before any work on it begins\n",
    "df_all_features.head(10)"
   ]
  },
  {
   "source": [
    "# Training the models"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Split the dataframe into X and y as numpy arrays"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "normalize the data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "to_scale = [\n",
    "    'AMT_INCOME_TOTAL',\n",
    "    'AMT_CREDIT',\n",
    "    'DAYS_BIRTH',\n",
    "    'DAYS_EMPLOYED',\n",
    "    'AMT_DOWN_PAYMENT'\n",
    "]\n",
    "subframe = df_all_features[to_scale].copy()\n",
    "scaler = preprocessing.MinMaxScaler().fit(subframe.values)\n",
    "subframe = scaler.transform(subframe.values)\n",
    "df_all_features[to_scale] = subframe\n",
    "df_final = df_all_features"
   ]
  },
  {
   "source": [
    "convert the dataframe into a numpy tensor"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = np.array(df_final)\n",
    "\n",
    "#undersample the target 1's so that the dataset is exactly balanced\n",
    "num_zero = len(data[data[:,0]==0])\n",
    "num_one = num_zero\n",
    "\n",
    "#balance the amount of approvals and rejections\n",
    "data = np.vstack((\n",
    "    data[data[:,0]==1][:num_one,:],\n",
    "    data[data[:,0]==0]\n",
    "))\n",
    "np.random.shuffle(data)\n",
    "\n",
    "X = data[:,1:]\n",
    "y = data[:,0].astype(np.int32)"
   ]
  },
  {
   "source": [
    "## Creating the test class\n",
    "We will create a class that collects everything we need to build and to analyze a model. The class will point to our data, and to a model building algorithm, and will provide functionality for presenting results."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "class Test:\n",
    "    def __init__(self, name, X, y, algorithm, args):\n",
    "        self.name = name\n",
    "        self.X = X  #pointer, not copy\n",
    "        self.y = y\n",
    "        self.algorithm = algorithm\n",
    "        self.args = args\n",
    "        \n",
    "        self.predictions=None\n",
    "        self.accs_train=None\n",
    "        self.accs_test=None\n",
    "        self.best_acc_index=None\n",
    "        self.models = None\n",
    "    \n",
    "    def get_args(self):\n",
    "        return dict(self.args)\n",
    "    \n",
    "    def get_models(self):\n",
    "        if self.models is None:\n",
    "            raise Exception('test has not yet been run')\n",
    "        return list(self.models)\n",
    "    \n",
    "    def get_model(self):\n",
    "        return self.get_models()[self.best_acc_index]\n",
    "    \n",
    "    def get_training_accuracies(self):\n",
    "        if self.accs_train is None:\n",
    "            raise Exception('test has not yet been run')\n",
    "        return list(self.accs_train)\n",
    "\n",
    "    def get_best_accuracy_training(self):\n",
    "        return self.get_training_accuracies()[self.best_acc_index]\n",
    "\n",
    "    def get_testing_accuracies(self):\n",
    "        if self.accs_test is None:\n",
    "            raise Exception('test has not yet been run')\n",
    "        return list(self.accs_test)\n",
    "    \n",
    "    def get_best_accuracy(self):\n",
    "        return self.get_testing_accuracies()[self.best_acc_index]\n",
    "\n",
    "    def get_predictions(self):\n",
    "        if self.predictions is None:\n",
    "            raise Exception('test has not yet been run')\n",
    "        return list(self.predictions)\n",
    "    \n",
    "    def get_best_prediction(self):\n",
    "        return self.get_predictions()[self.best_acc_index]\n",
    "    \n",
    "    #obtain accuracy using k-fold cross validation\n",
    "    def run(self, k_fold_splits=5):\n",
    "\n",
    "        predictions = []\n",
    "        accs_train = []\n",
    "        accs_test = []\n",
    "        best_acc_test = 0\n",
    "        best_acc_index = 0\n",
    "        models = []\n",
    "        \n",
    "        kfold_model = KFold(n_splits=k_fold_splits, random_state=None, shuffle=False)\n",
    "        \n",
    "        i = 0\n",
    "        for train_index, test_index in kfold_model.split(self.X):\n",
    "            X_train = self.X[train_index]\n",
    "            y_train = self.y[train_index]\n",
    "            X_test = self.X[test_index]\n",
    "            y_test = self.y[test_index]\n",
    "            model = self.algorithm(**self.args)\n",
    "            model.fit(X_train, y_train)\n",
    "            models.append(model)\n",
    "            accs_train.append(model.score(X_train,y_train))\n",
    "            acc_test = model.score(X_test,y_test)\n",
    "            accs_test.append(acc_test)\n",
    "            if acc_test > best_acc_test:\n",
    "                best_acc_test = acc_test\n",
    "                best_acc_index = i\n",
    "            predictions.append(model.predict(X_test))\n",
    "            i += 1\n",
    "        \n",
    "        self.accs_test = accs_test\n",
    "        self.predictions = predictions\n",
    "        self.accs_train = accs_train\n",
    "        self.best_acc_index = best_acc_index\n",
    "        self.models = models\n",
    "\n",
    "    \n",
    "    def display(self):\n",
    "        print(f'\\n\\n{self.name}\\n' + '='*len(self.name))\n",
    "        acc_train = self.get_best_accuracy_training()\n",
    "        acc_test = self.get_best_accuracy()\n",
    "        prediction = self.get_best_prediction()\n",
    "        print(f'Mean prediction: {np.round(np.mean(np.array(prediction)), 2)}')\n",
    "        print(f'Training accuracy: {acc_train}')\n",
    "        print(f'Test accuracy: {acc_test}')\n",
    "    \n",
    "    def plot(self):\n",
    "        pass"
   ]
  },
  {
   "source": [
    "## Support Vector Machine Modelling\n",
    "The motivation behind support vector machines is that we are building a line of best fit between two datasets, where \"best\" is defined by an objective function of distance between our line of best fit and between critical points, called support vectors, of these datasets. Support vectors are the closest points to a line of best fit. Our best fit line is also a decision boundary."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm"
   ]
  },
  {
   "source": [
    "svm_linear_tests = [\n",
    "    Test(\n",
    "        name=f'SVM linear with C of {i}',\n",
    "        X=X,\n",
    "        y=y,\n",
    "        algorithm=svm.SVC,\n",
    "        args={'kernel': 'linear', 'C': i}\n",
    "    )\n",
    "    for i in [0.001, 0.01, 0.1, 1.0/len(X), 1, 10]\n",
    "]\n",
    "\n",
    "for test in svm_linear_tests:\n",
    "    test.run()\n",
    "    test.display()"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "source": [
    "svm_rbf_tests = [\n",
    "    Test(\n",
    "        name=f'SVM rbf of with C of {i}',\n",
    "        X=X,\n",
    "        y=y,\n",
    "        algorithm=svm.SVC,\n",
    "        args={'kernel': 'rbf', 'C': i}\n",
    "    )\n",
    "    for i in [0.001, 0.01, 0.1, 1.0/len(X), 1, 10]\n",
    "]\n",
    "\n",
    "for test in svm_rbf_tests:\n",
    "    test.run()\n",
    "    test.display()"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "source": [
    "pairs = []\n",
    "for degree in range(1,6):\n",
    "    for C in [0.01, 0.1, 1.0/len(X), 1]:\n",
    "        pairs.append((degree, C))\n",
    "\n",
    "svm_poly_tests = [\n",
    "    Test(\n",
    "        name=f'SVM poly of degree {degree} and with C of {C}',\n",
    "        X=X,\n",
    "        y=y,\n",
    "        algorithm=svm.SVC,\n",
    "        args={'kernel': 'poly', 'C': C, 'degree': degree}\n",
    "    )\n",
    "    for (degree, C) in pairs\n",
    "]\n",
    "\n",
    "for test in svm_poly_tests:\n",
    "    test.run()\n",
    "    test.display()"
   ],
   "cell_type": "code",
   "metadata": {
    "tags": []
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "source": [
    "## Logistic Regression Modelling\n",
    "The motivation behind Logistic Regression modelling is that we attempt to *explain*, by defining a **likely** decision boundary, why a specific dataset is split the way it is. Our objective function is best when it is most likely the explanation for the given dataset split. We avoid overfitting by introducing a penalty function that is some function of weights that is then scaled by a ∆."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore', category=ConvergenceWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_l1_tests = [\n",
    "    Test(\n",
    "        name=f'Logreg l1 penalty with ∆={i}',\n",
    "        X=X,\n",
    "        y=y,\n",
    "        algorithm=linear_model.LogisticRegression,\n",
    "        args={'penalty':'l1', 'solver':'saga', 'C':i}\n",
    "    )\n",
    "    for i in [0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
    "]\n",
    "\n",
    "for test in logreg_l1_tests:\n",
    "    test.run()\n",
    "    test.display()"
   ]
  },
  {
   "source": [],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_l2_tests = [\n",
    "    Test(\n",
    "        name=f'Logreg l2 penalty with ∆={i}',\n",
    "        X=X,\n",
    "        y=y,\n",
    "        algorithm=linear_model.LogisticRegression,\n",
    "        args={'penalty':'l2', 'solver':'saga', 'C':i}\n",
    "    )\n",
    "    for i in [0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
    "]\n",
    "\n",
    "for test in logreg_l2_tests:\n",
    "    test.run()\n",
    "    test.display()"
   ]
  },
  {
   "source": [
    "## Neural Network Modelling\n",
    "The motvation behind neural network modelling is that we attempt to define abstractions that are functions of other abstractions that are functions of our input. At a high level, a neural network is a set of functions that are defined as computation graphs. To improve our function, we find the gradient of our augmented error function that includes a penalty function, just as in logistic regression."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code reference: https://scikit-learn.org/stable/modules/neural_networks_supervised.html\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_sgd_tests = [\n",
    "    Test(\n",
    "        name='NN using stochastic gradient descent and logistic activation with small ∆',\n",
    "        X=X,\n",
    "        y=y,\n",
    "        algorithm=MLPClassifier,\n",
    "        args={'solver': 'sgd', 'activation':'logistic', 'alpha': 1e-5, 'hidden_layer_sizes': (6,4,2), 'random_state': 52}\n",
    "    ),\n",
    "    Test(\n",
    "        name='NN using stochastic gradient descent and logistic activation with large ∆',\n",
    "        X=X,\n",
    "        y=y,\n",
    "        algorithm=MLPClassifier,\n",
    "        args={'solver': 'sgd', 'activation':'logistic', 'alpha': 10, 'hidden_layer_sizes': (6,4,2), 'random_state': 1245}\n",
    "    ),\n",
    "    Test(\n",
    "        name='NN using stochastic gradient descent and relu activation with small ∆',\n",
    "        X=X,\n",
    "        y=y,\n",
    "        algorithm=MLPClassifier,\n",
    "        args={'solver': 'sgd', 'activation':'relu', 'alpha': 1e-8, 'hidden_layer_sizes': (5,2), 'random_state': 734}\n",
    "    ),\n",
    "    Test(\n",
    "        name='NN using stochastic gradient descent and relu activation with large ∆',\n",
    "        X=X,\n",
    "        y=y,\n",
    "        algorithm=MLPClassifier,\n",
    "        args={'solver': 'sgd', 'activation':'relu', 'alpha': 1, 'hidden_layer_sizes': (5,2), 'random_state': 734}\n",
    "    )\n",
    "]\n",
    "\n",
    "for test in nn_sgd_tests:\n",
    "    test.run()\n",
    "    test.display()"
   ]
  },
  {
   "source": [
    "lambdas = [\n",
    "    1e-4,\n",
    "    1,\n",
    "    1e2,\n",
    "]\n",
    "structures = [\n",
    "    (4,3),\n",
    "    (9,6),\n",
    "    (9,6,3),\n",
    "    (10,6),\n",
    "    (10,8,6,4,2)\n",
    "]\n",
    "pairs = []\n",
    "for structure in structures:\n",
    "    for lmbda in lambdas:\n",
    "        pairs.append((structure, lmbda))\n",
    "\n",
    "nn_logistic_tests = [\n",
    "    Test(\n",
    "        name=f'NN with hidden layers of {structure} and with lmbda of {lmbda} and with activation of logistic function',\n",
    "        X=X,\n",
    "        y=y,\n",
    "        algorithm=MLPClassifier,\n",
    "        args={\n",
    "            'solver': 'lbfgs',\n",
    "            'alpha': lmbda,\n",
    "            'activation': 'logistic',\n",
    "            'hidden_layer_sizes': structure,\n",
    "            'random_state': 1\n",
    "        }\n",
    "    )\n",
    "    for (structure, lmbda) in pairs\n",
    "]\n",
    "\n",
    "for test in nn_logistic_tests:\n",
    "    test.run()\n",
    "    test.display()"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = []\n",
    "lambdas = [\n",
    "    1e-4,\n",
    "    1,\n",
    "    1e2,\n",
    "]\n",
    "random_weights = [i*17 for i in range(1,7)]\n",
    "\n",
    "structure = (4,3) #best results in previous demo\n",
    "\n",
    "for rw in random_weights:\n",
    "    for lmbda in lambdas:\n",
    "        pairs.append((rw,lmbda))\n",
    "\n",
    "nn_relu_tests = [\n",
    "    Test(\n",
    "        name=f'NN with hidden layers = {structure}; lmbda = {lmbda}; random weight seed = {rw}; activation = relu',\n",
    "        X=X,\n",
    "        y=y,\n",
    "        algorithm=MLPClassifier,\n",
    "        args={\n",
    "            'solver': 'lbfgs',\n",
    "            'alpha': lmbda,\n",
    "            'activation': 'relu',\n",
    "            'hidden_layer_sizes': structure,\n",
    "            'random_state': rw\n",
    "        }\n",
    "    )\n",
    "    for (rw, lmbda) in pairs\n",
    "]\n",
    "\n",
    "for test in nn_relu_tests:\n",
    "    test.run()\n",
    "    test.display()"
   ]
  }
 ]
}