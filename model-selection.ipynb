{
 "metadata": {
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python394jvsc74a57bd0daf19b0280700678d1b30283c052a0b4f397f6934ace6c653c7a54482fc82c11",
   "display_name": "Python 3.9.4 64-bit ('venv')"
  },
  "metadata": {
   "interpreter": {
    "hash": "9518128f597d7b00dc14729602cfd87fb7b2cf75925976bcb0d0e328a830a12b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Imports"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "source": [
    "# Data Cleaning"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Selecting Relevant Fields\n",
    "the dataset will be loaded and transformed and relevant dimensions will remain"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "  NAME_CONTRACT_STATUS CODE_GENDER FLAG_OWN_CAR FLAG_OWN_REALTY  CNT_CHILDREN  \\\n",
       "0             Approved           F            N               Y             0   \n",
       "1             Approved           F            Y               Y             1   \n",
       "2             Approved           F            N               Y             0   \n",
       "3             Approved           M            N               N             0   \n",
       "4              Refused           M            Y               N             0   \n",
       "5              Refused           M            Y               N             1   \n",
       "6             Approved           F            N               Y             1   \n",
       "7             Approved           M            N               N             0   \n",
       "8             Approved           F            N               N             0   \n",
       "9             Approved           M            N               Y             0   \n",
       "\n",
       "   AMT_INCOME_TOTAL  AMT_CREDIT      NAME_INCOME_TYPE  \\\n",
       "0          171000.0    491580.0         State servant   \n",
       "1          175500.0     29700.0         State servant   \n",
       "2          135000.0     48600.0               Working   \n",
       "3          180000.0    196740.0               Working   \n",
       "4          225000.0    774229.5             Pensioner   \n",
       "5          225000.0     36166.5               Working   \n",
       "6           90000.0    120582.0             Pensioner   \n",
       "7          135000.0     30550.5               Working   \n",
       "8           54000.0    112500.0             Pensioner   \n",
       "9          315000.0     26811.0  Commercial associate   \n",
       "\n",
       "             NAME_EDUCATION_TYPE NAME_FAMILY_STATUS  NAME_HOUSING_TYPE  \\\n",
       "0  Secondary / secondary special            Married  House / apartment   \n",
       "1               Higher education            Married  House / apartment   \n",
       "2  Secondary / secondary special            Married  House / apartment   \n",
       "3  Secondary / secondary special            Married   Rented apartment   \n",
       "4  Secondary / secondary special            Married  House / apartment   \n",
       "5  Secondary / secondary special            Married  House / apartment   \n",
       "6  Secondary / secondary special            Married  House / apartment   \n",
       "7  Secondary / secondary special            Married  House / apartment   \n",
       "8  Secondary / secondary special              Widow  House / apartment   \n",
       "9  Secondary / secondary special            Married  House / apartment   \n",
       "\n",
       "   DAYS_BIRTH  DAYS_EMPLOYED        OCCUPATION_TYPE  CNT_FAM_MEMBERS  \n",
       "0      -14548          -1187         Medicine staff              2.0  \n",
       "1      -11081          -3244  High skill tech staff              3.0  \n",
       "2      -12939           -629            Sales staff              2.0  \n",
       "3       -8945           -672            Sales staff              2.0  \n",
       "4      -23919         365243                    NaN              2.0  \n",
       "5      -15173          -3397                Drivers              3.0  \n",
       "6      -18834         365243                    NaN              3.0  \n",
       "7       -9950           -146               Laborers              2.0  \n",
       "8      -23154         365243                    NaN              1.0  \n",
       "9      -17154          -4006                Drivers              2.0  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>NAME_CONTRACT_STATUS</th>\n      <th>CODE_GENDER</th>\n      <th>FLAG_OWN_CAR</th>\n      <th>FLAG_OWN_REALTY</th>\n      <th>CNT_CHILDREN</th>\n      <th>AMT_INCOME_TOTAL</th>\n      <th>AMT_CREDIT</th>\n      <th>NAME_INCOME_TYPE</th>\n      <th>NAME_EDUCATION_TYPE</th>\n      <th>NAME_FAMILY_STATUS</th>\n      <th>NAME_HOUSING_TYPE</th>\n      <th>DAYS_BIRTH</th>\n      <th>DAYS_EMPLOYED</th>\n      <th>OCCUPATION_TYPE</th>\n      <th>CNT_FAM_MEMBERS</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Approved</td>\n      <td>F</td>\n      <td>N</td>\n      <td>Y</td>\n      <td>0</td>\n      <td>171000.0</td>\n      <td>491580.0</td>\n      <td>State servant</td>\n      <td>Secondary / secondary special</td>\n      <td>Married</td>\n      <td>House / apartment</td>\n      <td>-14548</td>\n      <td>-1187</td>\n      <td>Medicine staff</td>\n      <td>2.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Approved</td>\n      <td>F</td>\n      <td>Y</td>\n      <td>Y</td>\n      <td>1</td>\n      <td>175500.0</td>\n      <td>29700.0</td>\n      <td>State servant</td>\n      <td>Higher education</td>\n      <td>Married</td>\n      <td>House / apartment</td>\n      <td>-11081</td>\n      <td>-3244</td>\n      <td>High skill tech staff</td>\n      <td>3.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Approved</td>\n      <td>F</td>\n      <td>N</td>\n      <td>Y</td>\n      <td>0</td>\n      <td>135000.0</td>\n      <td>48600.0</td>\n      <td>Working</td>\n      <td>Secondary / secondary special</td>\n      <td>Married</td>\n      <td>House / apartment</td>\n      <td>-12939</td>\n      <td>-629</td>\n      <td>Sales staff</td>\n      <td>2.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Approved</td>\n      <td>M</td>\n      <td>N</td>\n      <td>N</td>\n      <td>0</td>\n      <td>180000.0</td>\n      <td>196740.0</td>\n      <td>Working</td>\n      <td>Secondary / secondary special</td>\n      <td>Married</td>\n      <td>Rented apartment</td>\n      <td>-8945</td>\n      <td>-672</td>\n      <td>Sales staff</td>\n      <td>2.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Refused</td>\n      <td>M</td>\n      <td>Y</td>\n      <td>N</td>\n      <td>0</td>\n      <td>225000.0</td>\n      <td>774229.5</td>\n      <td>Pensioner</td>\n      <td>Secondary / secondary special</td>\n      <td>Married</td>\n      <td>House / apartment</td>\n      <td>-23919</td>\n      <td>365243</td>\n      <td>NaN</td>\n      <td>2.0</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>Refused</td>\n      <td>M</td>\n      <td>Y</td>\n      <td>N</td>\n      <td>1</td>\n      <td>225000.0</td>\n      <td>36166.5</td>\n      <td>Working</td>\n      <td>Secondary / secondary special</td>\n      <td>Married</td>\n      <td>House / apartment</td>\n      <td>-15173</td>\n      <td>-3397</td>\n      <td>Drivers</td>\n      <td>3.0</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>Approved</td>\n      <td>F</td>\n      <td>N</td>\n      <td>Y</td>\n      <td>1</td>\n      <td>90000.0</td>\n      <td>120582.0</td>\n      <td>Pensioner</td>\n      <td>Secondary / secondary special</td>\n      <td>Married</td>\n      <td>House / apartment</td>\n      <td>-18834</td>\n      <td>365243</td>\n      <td>NaN</td>\n      <td>3.0</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>Approved</td>\n      <td>M</td>\n      <td>N</td>\n      <td>N</td>\n      <td>0</td>\n      <td>135000.0</td>\n      <td>30550.5</td>\n      <td>Working</td>\n      <td>Secondary / secondary special</td>\n      <td>Married</td>\n      <td>House / apartment</td>\n      <td>-9950</td>\n      <td>-146</td>\n      <td>Laborers</td>\n      <td>2.0</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>Approved</td>\n      <td>F</td>\n      <td>N</td>\n      <td>N</td>\n      <td>0</td>\n      <td>54000.0</td>\n      <td>112500.0</td>\n      <td>Pensioner</td>\n      <td>Secondary / secondary special</td>\n      <td>Widow</td>\n      <td>House / apartment</td>\n      <td>-23154</td>\n      <td>365243</td>\n      <td>NaN</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>Approved</td>\n      <td>M</td>\n      <td>N</td>\n      <td>Y</td>\n      <td>0</td>\n      <td>315000.0</td>\n      <td>26811.0</td>\n      <td>Commercial associate</td>\n      <td>Secondary / secondary special</td>\n      <td>Married</td>\n      <td>House / apartment</td>\n      <td>-17154</td>\n      <td>-4006</td>\n      <td>Drivers</td>\n      <td>2.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 123
    }
   ],
   "source": [
    "#TODO: remove the nrows argument when done testing\n",
    "df = pd.read_csv('database.csv', nrows=2500)[[\n",
    "    'NAME_CONTRACT_STATUS',\n",
    "    'CODE_GENDER',\n",
    "    'FLAG_OWN_CAR',\n",
    "    'FLAG_OWN_REALTY',\n",
    "    'CNT_CHILDREN',\n",
    "    'AMT_INCOME_TOTAL',\n",
    "    'AMT_CREDIT',\n",
    "    # 'NAME_INCOME_TYPE',\n",
    "    # 'NAME_EDUCATION_TYPE',\n",
    "    # 'NAME_FAMILY_STATUS',\n",
    "    # 'NAME_HOUSING_TYPE',\n",
    "    'DAYS_BIRTH',\n",
    "    'DAYS_EMPLOYED',\n",
    "    # 'OCCUPATION_TYPE',\n",
    "    'CNT_FAM_MEMBERS'\n",
    "]]\n",
    "\n",
    "# keep only approved and refused examples (then reset the pandas row indices - which does not happen automatically by default)\n",
    "df = df.loc[(df['NAME_CONTRACT_STATUS'] == 'Approved') | (df['NAME_CONTRACT_STATUS'] == 'Refused')].reset_index().drop(labels=['index'],axis=1)\n",
    "\n",
    "df.head(10)"
   ]
  },
  {
   "source": [
    "## Reductions to Binary Variables\n",
    "\n",
    "The target variable, NAME_CONTRACT_STATUS, will be reduced from one of 4 possible values, to one of two generic but still correct values â€“ for example, the dataset distingushes between cancelled and rejected and granted loans, however we will only distinguish between granted and not granted loans. Values that are binary but that don't use the binary alphabet will be transformed to use the binary alphabet as well.\n",
    "\n",
    "Defining a function that will return a copy of the dataframe with reduced fields."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce(df, name, value):\n",
    "    if type(name) != str:\n",
    "        raise Exception('only one dimension is reduced at a time')\n",
    "    idx_name = df.columns.get_loc(name)\n",
    "    reduced = [(1 if df[name][i] == value else 0) for i in range(len(df))]\n",
    "    df_reduced = df.drop(labels=[name], axis=1)\n",
    "    df_reduced.insert(loc=idx_name, column=name, value=reduced)\n",
    "    return df_reduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "df0 = reduce(df, 'NAME_CONTRACT_STATUS', 'Approved')  # 1 if approved else 0\n",
    "df1 = reduce(df0, 'CODE_GENDER', 'M')  # 1 if male else 0\n",
    "df2 = reduce(df1, 'FLAG_OWN_CAR', 'Y')  # 1 if owns car else 0\n",
    "df3 = reduce(df2, 'FLAG_OWN_REALTY', 'Y')  # 1 if owns property else 0"
   ]
  },
  {
   "source": [
    "## One-Hot Encoding\n",
    "defining a function that returns a copy of the input dataframe with a specific dimension one-hot encoded"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def one_hot_encode(df, name):\n",
    "    if type(name) != str:\n",
    "        raise Exception('one hot encoding applies to one dimension at a time')\n",
    "    if len(df) == 0:\n",
    "        raise Exception('dataframe is empty')\n",
    "    \n",
    "    df = df.copy()\n",
    "    values = df[name].unique()\n",
    "    \n",
    "    #for each unique value, we create a new column where df[row][new column] is 1 if the value of df[row][value] == new column\n",
    "    for v in values:\n",
    "        one_hot_column = [(1 if df[name][i] == v else 0) for i in range(len(df))]\n",
    "        df.insert(loc=len(df.loc[0]), column=v, value=one_hot_column)\n",
    "\n",
    "    return df.drop(labels=[name], axis=1)"
   ]
  },
  {
   "source": [
    "performing one-hot encoding on any dimension whose values are one of a set of string values"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df4 = one_hot_encode(df3, 'NAME_INCOME_TYPE')\n",
    "# df5 = one_hot_encode(df4, 'NAME_EDUCATION_TYPE')\n",
    "# df6 = one_hot_encode(df5, 'NAME_FAMILY_STATUS')\n",
    "# df7 = one_hot_encode(df6, 'NAME_HOUSING_TYPE')\n",
    "# df8 = one_hot_encode(df7, 'OCCUPATION_TYPE')\n",
    "df8=df3"
   ]
  },
  {
   "source": [
    "# Training the models"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Split the dataframe into X and y as numpy arrays"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "convert the dataframe into a numpy tensor"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "409 0.5\n"
     ]
    }
   ],
   "source": [
    "data = np.array(df8)\n",
    "\n",
    "#undersample the target 1's\n",
    "num_zero = len(data[data[:,0]==0])\n",
    "num_one = num_zero\n",
    "\n",
    "#balance the amount of approvals and rejections\n",
    "data = np.vstack((\n",
    "    data[data[:,0]==1][:num_one,:],\n",
    "    data[data[:,0]==0]\n",
    "))\n",
    "print(num_zero, num_one/float(len(data)))\n",
    "\n",
    "X = data[:,2:]\n",
    "y = data[:,1].reshape((-1,1)).astype(np.int32).ravel()"
   ]
  },
  {
   "source": [
    "normalize the data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'State servant'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-129-877d2aff32c5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpreprocessing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMinMaxScaler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/assignments/machine-learning/project/venv/lib/python3.9/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    697\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    698\u001b[0m             \u001b[0;31m# fit method of arity 1 (unsupervised transformation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 699\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    700\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m             \u001b[0;31m# fit method of arity 2 (supervised transformation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/assignments/machine-learning/project/venv/lib/python3.9/site-packages/sklearn/preprocessing/_data.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    361\u001b[0m         \u001b[0;31m# Reset internal state before fitting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 363\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartial_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpartial_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/assignments/machine-learning/project/venv/lib/python3.9/site-packages/sklearn/preprocessing/_data.py\u001b[0m in \u001b[0;36mpartial_fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    394\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m         \u001b[0mfirst_pass\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'n_samples_seen_'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 396\u001b[0;31m         X = self._validate_data(X, reset=first_pass,\n\u001b[0m\u001b[1;32m    397\u001b[0m                                 \u001b[0mestimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFLOAT_DTYPES\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m                                 force_all_finite=\"allow-nan\")\n",
      "\u001b[0;32m~/assignments/machine-learning/project/venv/lib/python3.9/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    419\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    420\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'no_validation'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 421\u001b[0;31m             \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    422\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/assignments/machine-learning/project/venv/lib/python3.9/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;31m# extra_args > 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/assignments/machine-learning/project/venv/lib/python3.9/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[1;32m    614\u001b[0m                     \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcasting\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"unsafe\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    615\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 616\u001b[0;31m                     \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    617\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mComplexWarning\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcomplex_warning\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    618\u001b[0m                 raise ValueError(\"Complex data not supported\\n\"\n",
      "\u001b[0;32m~/assignments/machine-learning/project/venv/lib/python3.9/site-packages/numpy/core/_asarray.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order, like)\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_asarray_with_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlike\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlike\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: could not convert string to float: 'State servant'"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "X = preprocessing.MinMaxScaler().fit_transform(X)"
   ]
  },
  {
   "source": [
    "## Creating the test class\n",
    "We will create a class that collects everything we need to build and to analyze a model. The class will point to our data, and to a model building algorithm, and will provide functionality for presenting results."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "class Test:\n",
    "    def __init__(self, name, X, y, algorithm, args):\n",
    "        self.name = name\n",
    "        self.X = X  #pointer, not copy\n",
    "        self.y = y\n",
    "        self.algorithm = algorithm\n",
    "        self.args = args\n",
    "        self.prediction=None\n",
    "        self.avg_acc_train=None\n",
    "        self.avg_acc_test=None\n",
    "    \n",
    "    def get_best_prediction(self):\n",
    "        if self.prediction is None:\n",
    "            raise Exception('the algorithm has not yet been run')\n",
    "        return self.prediction\n",
    "    \n",
    "    def get_avg_accuracies(self):\n",
    "        if self.avg_acc_train is None or self.avg_acc_test is None:\n",
    "            raise Exception('the algorithm has not yet been run')\n",
    "        return (self.avg_acc_train, self.avg_acc_test)\n",
    "    \n",
    "    def get_stats(self):\n",
    "        return (*self.get_avg_accuracies(), self.get_best_prediction())\n",
    "\n",
    "    #obtain accuracy using k-fold cross validation\n",
    "    def run(self, n_splits=5):\n",
    "        accs_train = []\n",
    "        accs_test = []\n",
    "        best_acc_test = 0\n",
    "        best_acc_index = -1\n",
    "        predictions = []\n",
    "        kfold_model = KFold(n_splits=n_splits, random_state=None, shuffle=False)\n",
    "        i = 0\n",
    "        for train_index, test_index in kfold_model.split(self.X):\n",
    "            X_train = self.X[train_index]\n",
    "            y_train = self.y[train_index]\n",
    "            X_test = self.X[test_index]\n",
    "            y_test = self.y[test_index]\n",
    "            model = self.algorithm(**self.args)\n",
    "            model.fit(X_train, y_train)\n",
    "            accs_train.append(model.score(X_train,y_train))\n",
    "            acc_test = model.score(X_test,y_test)\n",
    "            accs_test.append(acc_test)\n",
    "            if acc_test > best_acc_test:\n",
    "                best_acc_test = acc_test\n",
    "                best_acc_index = i\n",
    "            predictions.append(model.predict(X_test))\n",
    "            i += 1\n",
    "        self.avg_acc_train = sum(accs_train)/float(len(accs_train))\n",
    "        self.avg_acc_test = sum(accs_test)/float(len(accs_test))\n",
    "        self.prediction = predictions[best_acc_index]\n",
    "        return self.get_stats()\n",
    "    \n",
    "    def display(self):\n",
    "        print(f'\\n\\n{self.name}\\n' + '='*len(self.name))\n",
    "        print(f'Average training accuracy: {self.avg_acc_train}')\n",
    "        print(f'Average test accuracy: {self.avg_acc_test}')\n",
    "        print(f'Best prediction: {self.prediction}')"
   ]
  },
  {
   "source": [
    "## Support Vector Machine Modelling\n",
    "The motivation behind support vector machines is that we are building a line of best fit between two datasets, where \"best\" is defined by an objective function of distance between our line of best fit and between critical points, called support vectors, of these datasets. Support vectors are the closest points to a line of best fit. Our best fit line is also a decision boundary."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm"
   ]
  },
  {
   "source": [
    "### Tests comparing the three kernels"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "\n",
      "SVM linear\n",
      "==========\n",
      "Average training accuracy: 0.7839223101524382\n",
      "Average test accuracy: 0.7421068382462965\n",
      "Best prediction: [1 1 1 1 1 1 0 0 0 1 1 1 1 1 0 0 0 0 1 1 0 1 1 1 0 0 0 0 0 1 1 0 1 0 1 0 0\n",
      " 0 0 1 0 0 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 0 1 0 0 1 1 1 0 0 0\n",
      " 1 1 0 1 0 0 0 1 1 0 1 1 0 1 1 0 0 1 1 0 0 0 0 1 1 1 0 1 1 0 0 0 1 1 0 0 0\n",
      " 0 0 1 0 0 1 0 1 1 1 1 0 1 0 0 0 1 0 1 1 1 1 0 1 1 0 1 1 1 0 0 0 1 0 0 1 1\n",
      " 0 0 1 1 0 0 1 1 1 0 1 0 0 0 1]\n",
      "\n",
      "\n",
      "SVM rbf\n",
      "=======\n",
      "Average training accuracy: 0.8783598291196864\n",
      "Average test accuracy: 0.7567335029178512\n",
      "Best prediction: [0 0 1 0 0 1 1 1 0 0 0 0 0 1 1 1 1 1 0 1 1 0 1 0 1 1 0 0 0 1 1 0 1 0 0 0 0\n",
      " 0 0 0 0 1 0 0 0 1 0 1 1 1 1 1 0 0 0 0 0 1 1 0 0 1 0 0 0 1 1 1 1 1 0 1 1 0\n",
      " 1 1 1 0 0 1 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 1 1 0 0 0 1 1\n",
      " 0 1 0 0 0 1 1 0 0 1 0 1 0 0 1 1 0 0 1 0 0 1 0 0 1 1 1 0 0 1 0 0 0 0 0 1 1\n",
      " 0 0 1 0 0 0 1 1 0 0 0 0 0 0 1 1]\n"
     ]
    }
   ],
   "source": [
    "svm_basic_tests = [\n",
    "    Test(\n",
    "        name='SVM linear',\n",
    "        X=X,\n",
    "        y=y,\n",
    "        algorithm=svm.SVC,\n",
    "        args={'kernel': 'linear', 'C': 10, 'class_weight': {0:1,1:2}}\n",
    "    ),\n",
    "    Test(\n",
    "        name='SVM rbf',\n",
    "        X=X,\n",
    "        y=y,\n",
    "        algorithm=svm.SVC,\n",
    "        args={'kernel': 'rbf', 'C': 10, 'class_weight': {0:1,1:2}}\n",
    "    ),\n",
    "]\n",
    "\n",
    "for test in svm_basic_tests:\n",
    "    test.run()\n",
    "    test.display()"
   ]
  },
  {
   "source": [
    "### Tests comparing different degrees of the polynomial kernel"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "\n",
      "SVM poly of degree 1\n",
      "====================\n",
      "Average training accuracy: 0.3997586198846791\n",
      "Average test accuracy: 0.39608708663773756\n",
      "Best prediction: [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 1 1 1 1 0 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 0 1 1 1 1 1 1 1 1 1 0 1 1 1]\n",
      "\n",
      "\n",
      "SVM poly of degree 2\n",
      "====================\n",
      "Average training accuracy: 0.5718206223591755\n",
      "Average test accuracy: 0.5660332186143947\n",
      "Best prediction: [1 1 1 1 1 1 0 1 0 1 1 1 1 1 1 1 0 0 1 1 0 1 0 0 0 0 1 0 0 1 1 0 1 1 1 0 1\n",
      " 0 1 1 0 1 1 1 1 1 1 0 1 1 0 0 0 0 1 0 1 0 0 1 1 1 1 1 0 1 0 0 1 1 1 1 0 0\n",
      " 0 1 0 1 0 1 0 1 1 0 1 1 0 1 1 0 0 1 1 0 0 0 1 1 1 1 0 1 1 0 0 1 1 1 1 1 0\n",
      " 1 1 1 0 1 1 0 1 1 0 1 0 1 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 0 1 1\n",
      " 0 0 1 1 0 0 1 1 1 0 1 0 1 0 1]\n",
      "\n",
      "\n",
      "SVM poly of degree 3\n",
      "====================\n",
      "Average training accuracy: 0.6913359945841212\n",
      "Average test accuracy: 0.6588433338321112\n",
      "Best prediction: [1 1 1 1 1 1 0 0 0 1 0 1 1 1 0 0 0 0 1 1 0 1 0 0 0 0 0 0 0 1 1 0 1 1 0 0 0\n",
      " 0 0 1 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 1 0 0 0 1 0 0 0 0\n",
      " 0 0 0 0 0 0 0 1 1 0 0 1 0 1 0 0 0 1 0 0 0 0 0 1 1 1 0 1 1 0 0 0 0 1 0 0 0\n",
      " 0 0 1 0 0 1 0 1 0 0 0 0 0 0 0 0 1 0 1 1 1 1 0 1 1 0 0 1 1 0 0 0 0 0 0 1 1\n",
      " 0 0 0 1 0 0 1 1 1 0 1 0 1 0 1]\n",
      "\n",
      "\n",
      "SVM poly of degree 4\n",
      "====================\n",
      "Average training accuracy: 0.7432892125965871\n",
      "Average test accuracy: 0.7175519976058656\n",
      "Best prediction: [0 0 0 0 0 1 0 0 0 1 1 0 0 0 0 0 1 0 1 0 1 0 0 1 0 1 0 0 0 1 1 0 0 0 0 0 0\n",
      " 0 0 0 0 0 1 0 0 1 0 0 0 0 1 1 0 1 1 0 0 0 1 0 0 1 1 1 0 0 1 0 0 0 0 0 1 0\n",
      " 0 0 0 1 1 0 0 0 1 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 1 0\n",
      " 1 1 0 0 0 0 1 0 0 0 0 0 0 1 0 0 1 0 0 1 0 0 1 0 0 0 0 0 0 0 1 1 0 0 0 1 1\n",
      " 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0]\n",
      "\n",
      "\n",
      "SVM poly of degree 5\n",
      "====================\n",
      "Average training accuracy: 0.7597805635315265\n",
      "Average test accuracy: 0.7359569055813259\n",
      "Best prediction: [0 0 0 0 1 1 1 0 0 0 0 1 1 1 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 1 1 0 0 0 1 0 0\n",
      " 0 0 0 0 1 1 1 0 0 1 0 1 0 0 0 0 0 0 0 1 0 0 0 1 1 0 1 0 1 0 0 0 1 0 1 0 1\n",
      " 0 0 0 0 1 0 0 0 0 0 1 1 0 1 1 1 0 1 1 0 1 0 0 0 0 1 0 1 0 1 1 0 0 0 1 0 1\n",
      " 1 1 0 0 0 0 1 1 0 1 0 0 0 1 1 1 0 0 0 0 1 0 0 1 1 0 0 1 0 1 1 0 1 0 0 0 0\n",
      " 1 0 0 0 1 0 0 0 0 0 0 0 1 1 1]\n",
      "\n",
      "\n",
      "SVM poly of degree 6\n",
      "====================\n",
      "Average training accuracy: 0.7597800966454233\n",
      "Average test accuracy: 0.7359643872512345\n",
      "Best prediction: [0 0 0 0 1 1 1 0 0 0 0 1 1 1 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 1 0 0 0 0 1 0 0\n",
      " 0 0 0 0 1 1 1 0 0 1 0 1 0 0 0 0 0 0 0 1 0 0 0 0 1 0 1 0 1 0 0 0 1 0 1 0 1\n",
      " 0 0 0 0 1 0 0 0 0 0 1 1 0 1 1 1 0 1 1 0 0 0 0 0 0 0 0 1 0 1 1 0 0 0 1 0 0\n",
      " 1 0 0 0 0 0 1 1 0 1 0 0 0 1 1 1 0 0 0 0 0 0 0 1 1 0 0 1 0 0 1 0 1 0 0 0 0\n",
      " 1 0 0 0 1 0 0 0 0 0 0 0 1 1 1]\n",
      "\n",
      "\n",
      "SVM poly of degree 7\n",
      "====================\n",
      "Average training accuracy: 0.7652823493708709\n",
      "Average test accuracy: 0.7384183749812958\n",
      "Best prediction: [0 0 0 0 1 1 1 0 0 0 0 1 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 1 0 0 0 0 1 0 0\n",
      " 0 0 0 0 1 1 1 0 0 1 0 1 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 1 0 0 0 1 0 1 0 1\n",
      " 0 0 0 0 1 0 0 0 0 0 1 1 0 0 0 1 0 1 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0\n",
      " 1 0 0 0 0 0 1 1 0 1 0 0 0 1 1 0 0 0 0 0 0 0 0 1 1 0 0 1 0 0 1 0 1 0 0 0 0\n",
      " 0 0 0 0 1 0 0 0 0 0 0 0 1 1 1]\n",
      "\n",
      "\n",
      "SVM poly of degree 8\n",
      "====================\n",
      "Average training accuracy: 0.7710894787216659\n",
      "Average test accuracy: 0.7359793505910519\n",
      "Best prediction: [0 0 0 0 1 1 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 1 0 0\n",
      " 0 0 0 0 1 1 1 0 0 1 0 1 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 1 0 0 0 1 0 1 0 1\n",
      " 0 0 0 0 1 1 0 0 0 0 1 1 0 0 0 1 0 1 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0\n",
      " 1 0 0 0 0 0 1 0 0 1 0 0 0 1 1 0 0 0 0 0 0 0 0 1 1 0 0 1 0 0 1 0 1 0 0 0 0\n",
      " 0 0 0 0 1 0 0 0 0 0 0 0 1 0 1]\n",
      "\n",
      "\n",
      "SVM poly of degree 9\n",
      "====================\n",
      "Average training accuracy: 0.7768952074141513\n",
      "Average test accuracy: 0.7298668262756246\n",
      "Best prediction: [0 0 0 0 1 1 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 1 0 0 0 0 1 0 0\n",
      " 0 0 0 0 1 1 1 0 0 1 0 1 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 1 0 0 0 1 0 1 0 1\n",
      " 0 0 0 0 1 1 0 0 0 0 1 1 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0\n",
      " 1 0 0 0 0 0 1 0 0 1 0 0 0 1 1 0 0 0 0 0 0 0 0 1 1 0 0 1 0 0 0 0 1 0 0 0 0\n",
      " 0 0 0 0 1 0 0 0 0 0 0 0 1 0 1]\n",
      "\n",
      "\n",
      "SVM poly of degree 10\n",
      "=====================\n",
      "Average training accuracy: 0.783616966640988\n",
      "Average test accuracy: 0.7347673200658387\n",
      "Best prediction: [0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 1 0 0 0 0 1 0 0\n",
      " 0 0 0 0 1 1 1 0 0 1 0 1 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 1 0 0 0 1 0 1 0 1\n",
      " 0 0 0 0 1 1 0 0 0 0 1 1 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0\n",
      " 1 0 0 0 0 0 1 0 0 1 0 0 0 1 1 0 0 0 0 0 0 0 0 1 1 0 0 1 0 0 0 0 1 0 0 0 0\n",
      " 0 0 0 0 1 0 0 0 0 0 0 0 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "svm_poly_tests = [\n",
    "    Test(\n",
    "        name=f'SVM poly of degree {i}',\n",
    "        X=X,\n",
    "        y=y,\n",
    "        algorithm=svm.SVC,\n",
    "        args={'class_weight': {0:1,1:2}, 'kernel': 'poly', 'C': 0.01, 'degree': i}\n",
    "    )\n",
    "    for i in range(1,11)\n",
    "]\n",
    "\n",
    "for test in svm_poly_tests:\n",
    "    test.run()\n",
    "    test.display()"
   ]
  },
  {
   "source": [
    "## Logistic Regression Modelling"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore', category=ConvergenceWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "\n",
      "Logreg l1 penalty with C=0.0001\n",
      "===============================\n",
      "Average training accuracy: 0.35085603567009827\n",
      "Average test accuracy: 0.3508603920395032\n",
      "Best prediction: [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "\n",
      "\n",
      "Logreg l1 penalty with C=0.001\n",
      "==============================\n",
      "Average training accuracy: 0.35085603567009827\n",
      "Average test accuracy: 0.3508603920395032\n",
      "Best prediction: [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "\n",
      "\n",
      "Logreg l1 penalty with C=0.01\n",
      "=============================\n",
      "Average training accuracy: 0.35085510189789204\n",
      "Average test accuracy: 0.34963339817447253\n",
      "Best prediction: [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "\n",
      "\n",
      "Logreg l1 penalty with C=0.1\n",
      "============================\n",
      "Average training accuracy: 0.4523570744916777\n",
      "Average test accuracy: 0.43385455633697434\n",
      "Best prediction: [1 0 1 1 0 1 1 0 0 1 0 1 0 1 1 1 0 1 1 1 1 1 0 1 1 1 1 1 1 0 1 1 0 1 0 1 1\n",
      " 1 1 1 1 1 1 0 1 1 1 0 1 0 1 1 1 1 1 1 0 0 0 0 1 0 0 1 0 1 1 1 1 1 0 1 1 1\n",
      " 1 1 1 0 1 1 0 1 1 1 1 1 1 1 1 0 1 0 1 1 1 1 1 1 1 0 1 1 1 1 1 0 1 0 1 1 1\n",
      " 0 0 1 1 1 1 1 0 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0\n",
      " 0 1 1 1 1 1 1 1 0 1 1 1 0 0 1 1]\n",
      "\n",
      "\n",
      "Logreg l1 penalty with C=1\n",
      "==========================\n",
      "Average training accuracy: 0.488730303242524\n",
      "Average test accuracy: 0.47056711057908124\n",
      "Best prediction: [1 0 1 0 0 0 1 0 1 1 0 1 0 1 1 0 0 0 1 1 1 1 0 0 0 1 0 1 1 1 1 0 0 1 0 1 1\n",
      " 1 1 1 1 0 1 0 1 0 1 0 1 0 1 1 1 1 1 1 0 0 0 1 1 0 0 1 1 1 1 1 1 0 0 0 1 1\n",
      " 0 1 1 0 1 1 1 0 1 0 1 1 1 1 1 0 1 0 1 0 0 0 0 0 1 0 1 1 0 1 1 0 0 0 0 0 1\n",
      " 0 0 1 1 1 0 1 0 0 1 0 1 0 1 1 0 1 0 0 1 1 0 1 0 1 1 0 0 1 0 0 1 0 1 0 0 0\n",
      " 0 1 0 1 1 1 0 0 0 0 1 0 0 0 1 1]\n",
      "\n",
      "\n",
      "Logreg l1 penalty with C=10\n",
      "===========================\n",
      "Average training accuracy: 0.5605163760300675\n",
      "Average test accuracy: 0.529373036061649\n",
      "Best prediction: [1 1 1 1 1 1 1 1 0 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 0 1 1 1 0 1 1 1 1 0 1 1 0\n",
      " 0 1 1 0 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0\n",
      " 1 1 0 1 0 1 1 1 1 0 1 1 1 1 1 1 0 1 1 0 1 0 1 1 1 1 1 1 1 0 1 1 1 1 1 0 1\n",
      " 0 0 1 1 0 1 0 1 1 1 1 1 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1\n",
      " 0 0 1 1 1 1 1 1 1 0 1 1 0 0 1]\n"
     ]
    }
   ],
   "source": [
    "logreg_l1_tests = [\n",
    "    Test(\n",
    "        name=f'Logreg l1 penalty with C={i}',\n",
    "        X=X,\n",
    "        y=y,\n",
    "        algorithm=linear_model.LogisticRegression,\n",
    "        args={'class_weight': {1:10}, 'penalty':'l1', 'solver':'saga', 'C':i}\n",
    "    )\n",
    "    for i in [0.0001, 0.001, 0.01, 0.1, 1, 10]\n",
    "]\n",
    "\n",
    "for test in logreg_l1_tests:\n",
    "    test.run()\n",
    "    test.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "\n",
      "Logreg l2 penalty with C=0.0001\n",
      "===============================\n",
      "Average training accuracy: 0.35085603567009827\n",
      "Average test accuracy: 0.3508603920395032\n",
      "Best prediction: [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "\n",
      "\n",
      "Logreg l2 penalty with C=0.001\n",
      "==============================\n",
      "Average training accuracy: 0.35085603567009827\n",
      "Average test accuracy: 0.3508603920395032\n",
      "Best prediction: [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "\n",
      "\n",
      "Logreg l2 penalty with C=0.01\n",
      "=============================\n",
      "Average training accuracy: 0.3759324882694866\n",
      "Average test accuracy: 0.37890917252730805\n",
      "Best prediction: [1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 0 1 0 1 1 1 0 1 1 1 1 0 1 1 1 1 0 0 0 1 1 0\n",
      " 1 0 1 1 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 0 1 1 1 1 1 0 1 1 1\n",
      " 0 0 1 1 1 1 1 1 1 1 1 0 0 0 1 1 1 1 0 1 1 1 1 1 0 1 1 1 1 1 0 1 1 1 1 1 1\n",
      " 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 0 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 0 0 0 1 1 0 1 0 1 1 1]\n",
      "\n",
      "\n",
      "Logreg l2 penalty with C=0.1\n",
      "============================\n",
      "Average training accuracy: 0.5899353362747158\n",
      "Average test accuracy: 0.5815801286847224\n",
      "Best prediction: [1 0 1 0 0 1 0 1 0 0 0 1 0 1 1 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 1 0 1 0 1\n",
      " 0 0 0 1 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 1 1 0 0 1 0 0 0 0 0 0 1 1 0 0 1 0\n",
      " 0 1 1 0 1 0 1 1 1 1 0 0 0 1 0 0 0 0 0 0 0 1 0 0 1 0 0 1 0 1 1 1 0 1 1 1 0\n",
      " 0 1 0 0 0 0 0 0 0 1 0 1 1 0 0 0 0 0 1 0 0 0 1 0 1 1 1 1 0 1 0 0 0 0 1 1 0\n",
      " 1 0 1 0 0 0 1 1 0 0 0 0 0 0 0 1]\n",
      "\n",
      "\n",
      "Logreg l2 penalty with C=1\n",
      "==========================\n",
      "Average training accuracy: 0.6125727758713262\n",
      "Average test accuracy: 0.5949797994912465\n",
      "Best prediction: [0 1 0 1 0 1 0 1 0 1 1 0 0 0 1 0 1 0 1 1 1 0 0 1 0 1 0 0 1 1 1 0 1 1 0 0 0\n",
      " 0 1 1 1 0 1 0 1 1 1 0 0 0 1 1 1 0 0 0 0 0 1 0 0 1 0 1 0 0 1 1 0 0 0 1 1 0\n",
      " 0 0 1 1 1 1 0 1 1 0 0 0 0 0 0 0 1 1 0 1 0 1 0 0 1 1 0 1 1 1 0 1 1 1 1 1 0\n",
      " 1 1 0 0 1 0 1 1 1 1 0 1 1 1 0 1 1 0 0 1 1 0 0 0 1 0 0 1 0 0 0 1 0 1 1 1 1\n",
      " 0 1 0 0 0 0 0 1 1 0 0 1 0 1 1 1]\n",
      "\n",
      "\n",
      "Logreg l2 penalty with C=10\n",
      "===========================\n",
      "Average training accuracy: 0.5770539486892172\n",
      "Average test accuracy: 0.5671330240909771\n",
      "Best prediction: [1 0 1 0 0 1 1 1 0 0 0 1 1 1 1 1 0 1 0 0 1 1 1 0 0 1 0 1 0 1 1 1 1 0 1 1 1\n",
      " 0 1 1 1 1 0 1 0 0 0 1 1 1 1 1 0 0 0 0 0 1 1 1 1 1 0 0 0 1 0 1 1 1 1 1 1 0\n",
      " 0 1 1 1 1 1 1 1 1 1 0 0 0 1 1 1 0 0 0 0 0 1 0 0 1 0 0 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 0 0 0 1 1 0 1 1 0 1 1 0 1 1 0 0 1 0 1 1 1 0 1 1 1 0 0 1 0 0 0 0 1 1 0\n",
      " 1 0 1 0 1 0 1 1 0 1 1 0 0 0 1 1]\n",
      "\n",
      "\n",
      "Logreg l2 penalty with C=100\n",
      "============================\n",
      "Average training accuracy: 0.6598921493101757\n",
      "Average test accuracy: 0.6465210234924436\n",
      "Best prediction: [0 1 0 1 0 1 0 1 0 1 1 0 0 1 1 0 1 0 1 1 1 0 0 1 0 1 1 0 1 1 1 1 1 1 1 0 0\n",
      " 0 1 1 1 0 1 0 1 1 1 0 0 0 1 1 1 0 1 0 0 1 1 0 0 1 0 1 0 0 1 1 0 0 0 1 1 0\n",
      " 0 0 1 1 1 1 0 1 1 0 0 0 0 0 0 0 1 0 0 1 0 1 0 0 1 0 0 1 1 0 0 1 0 0 1 1 0\n",
      " 1 1 1 0 1 0 1 0 1 1 0 1 0 1 0 1 1 0 0 1 0 0 0 0 1 0 0 1 0 0 0 1 1 0 1 1 1\n",
      " 0 1 0 0 0 0 0 1 1 0 0 1 1 0 1 1]\n",
      "\n",
      "\n",
      "Logreg l2 penalty with C=1000\n",
      "=============================\n",
      "Average training accuracy: 0.7023096855522095\n",
      "Average test accuracy: 0.6895555888074218\n",
      "Best prediction: [0 0 0 0 0 1 0 1 0 0 1 0 0 0 0 0 1 0 1 1 1 0 0 0 0 1 0 0 0 1 1 0 0 0 0 0 0\n",
      " 0 0 0 1 0 1 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0\n",
      " 0 0 0 1 0 0 0 1 1 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 1 0\n",
      " 1 1 0 0 1 0 1 0 0 1 0 1 0 1 0 1 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 1 0 1\n",
      " 0 0 0 0 0 0 0 0 1 0 0 1 0 0 1 0]\n"
     ]
    }
   ],
   "source": [
    "logreg_l2_tests = [\n",
    "    Test(\n",
    "        name=f'Logreg l2 penalty with C={i}',\n",
    "        X=X,\n",
    "        y=y,\n",
    "        algorithm=linear_model.LogisticRegression,\n",
    "        args={'class_weight': {1:10}, 'penalty':'l2', 'solver':'saga', 'C':i}\n",
    "    )\n",
    "    for i in [0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
    "]\n",
    "\n",
    "for test in logreg_l2_tests:\n",
    "    test.run()\n",
    "    test.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}