{
 "metadata": {
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python394jvsc74a57bd0daf19b0280700678d1b30283c052a0b4f397f6934ace6c653c7a54482fc82c11",
   "display_name": "Python 3.9.4 64-bit ('venv')"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Imports"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "source": [
    "# Data Cleaning"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Selecting Relevant Fields\n",
    "the dataset will be loaded and transformed and relevant dimensions will remain"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: remove the nrows argument when done testing\n",
    "df = pd.read_csv('database.csv', nrows=2500)[[\n",
    "    'NAME_CONTRACT_STATUS',\n",
    "    'CODE_GENDER',\n",
    "    'FLAG_OWN_CAR',\n",
    "    'FLAG_OWN_REALTY',\n",
    "    'CNT_CHILDREN',\n",
    "    'AMT_INCOME_TOTAL',\n",
    "    'AMT_CREDIT',\n",
    "    'NAME_INCOME_TYPE',\n",
    "    'NAME_EDUCATION_TYPE',\n",
    "    'NAME_FAMILY_STATUS',\n",
    "    'NAME_HOUSING_TYPE',\n",
    "    'DAYS_BIRTH',\n",
    "    'DAYS_EMPLOYED',\n",
    "    'OCCUPATION_TYPE',\n",
    "    'CNT_FAM_MEMBERS'\n",
    "]]"
   ]
  },
  {
   "source": [
    "## Reductions to binary variables\n",
    "\n",
    "The target variable, NAME_CONTRACT_STATUS, will be reduced from one of 4 possible values, to one of two generic but still correct values â€“ for example, the dataset distingushes between cancelled and rejected and granted loans, however we will only distinguish between granted and not granted loans. Values that are binary but that don't use the binary alphabet will be transformed to use the binary alphabet as well.\n",
    "\n",
    "Defining a function that will return a copy of the dataframe with reduced fields."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce(df, name, value):\n",
    "    if type(name) != str:\n",
    "        raise Exception('only one dimension is reduced at a time')\n",
    "    idx_name = df.columns.get_loc(name)\n",
    "    reduced = [(1 if df[name][i] == value else 0) for i in range(len(df))]\n",
    "    df_reduced = df.drop(labels=[name], axis=1)\n",
    "    df_reduced.insert(loc=idx_name, column=name, value=reduced)\n",
    "    return df_reduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "df0 = reduce(df, 'NAME_CONTRACT_STATUS', 'Approved')  # 1 if approved else 0\n",
    "df1 = reduce(df0, 'CODE_GENDER', 'M')  # 1 if male else 0\n",
    "df2 = reduce(df1, 'FLAG_OWN_CAR', 'Y')  # 1 if owns car else 0\n",
    "df3 = reduce(df2, 'FLAG_OWN_REALTY', 'Y')  # 1 if owns property else 0"
   ]
  },
  {
   "source": [
    "## One-Hot Encoding\n",
    "defining a function that returns a copy of the input dataframe with a specific dimension one-hot encoded"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def one_hot_encode(df, name):\n",
    "    if type(name) != str:\n",
    "        raise Exception('one hot encoding applies to one dimension at a time')\n",
    "    if len(df) == 0:\n",
    "        raise Exception('dataframe is empty')\n",
    "    \n",
    "    df = df.copy()\n",
    "    values = df[name].unique()\n",
    "    \n",
    "    #for each unique value, we create a new column where df[row][new column] is 1 if the value of df[row][value] == new column\n",
    "    for v in values:\n",
    "        one_hot_column = [(1 if df[name][i] == v else 0) for i in range(len(df))]\n",
    "        df.insert(loc=len(df.loc[0]), column=v, value=one_hot_column)\n",
    "\n",
    "    return df.drop(labels=[name], axis=1)"
   ]
  },
  {
   "source": [
    "performing one-hot encoding on any dimension whose values are one of a set of string values"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4 = one_hot_encode(df3, 'NAME_INCOME_TYPE')\n",
    "df5 = one_hot_encode(df4, 'NAME_EDUCATION_TYPE')\n",
    "df6 = one_hot_encode(df5, 'NAME_FAMILY_STATUS')\n",
    "df7 = one_hot_encode(df6, 'NAME_HOUSING_TYPE')\n",
    "df8 = one_hot_encode(df7, 'OCCUPATION_TYPE')"
   ]
  },
  {
   "source": [
    "# Training the models"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Split the dataframe into X and y as numpy arrays"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "convert the dataframe into a numpy tensor"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_as_array = np.array(df8)\n",
    "X = dataset_as_array[:,1:]\n",
    "y = dataset_as_array[:,0].reshape((-1,1))"
   ]
  },
  {
   "source": [
    "normalize the data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "X_norm = preprocessing.normalize(X)\n",
    "y_norm = preprocessing.normalize(y)"
   ]
  },
  {
   "source": [
    "## Creating the test class\n",
    "We will create a class that collects everything we need to build and to analyze a model. The class will point to our data, and to a model building algorithm, and will provide functionality for presenting results."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class Test:\n",
    "    def __init__(self, name, X, y, algorithm, args):\n",
    "        self.name = name\n",
    "        self.X = X  #pointer, not copy\n",
    "        self.y = y\n",
    "        self.algorithm = algorithm\n",
    "        self.args = args\n",
    "    \n",
    "    def run(self, split=0.8):\n",
    "        X_train, X_test = train_test_split(X_norm, train_size=split, test_size=(1.0-split))\n",
    "        y_train, y_test = train_test_split(y_norm, train_size=split, test_size=(1.0-split))\n",
    "        model = self.algorithm(**self.args)\n",
    "        model.fit(X_train, y_train.ravel())\n",
    "        acc_train = model.score(X_train,y_train)\n",
    "        acc_test = model.score(X_test,y_test)\n",
    "        return acc_train, acc_test"
   ]
  },
  {
   "source": [
    "## Support Vector Machine Modelling\n",
    "The motivation behind support vector machines is that we are building a line of best fit between two datasets, where \"best\" is defined by an objective function of distance between our line of best fit and between critical points, called support vectors, of these datasets. Support vectors are the closest points to a line of best fit. Our best fit line is also a decision boundary."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_tests = [\n",
    "    Test(\n",
    "        name='SVM linear',\n",
    "        X=X_norm,\n",
    "        y=y_norm,\n",
    "        algorithm=svm.SVC,\n",
    "        args={'probability': False, 'kernel': 'linear', 'C': 1}\n",
    "    ),\n",
    "    Test(\n",
    "        name='SVM rbf',\n",
    "        X=X_norm,\n",
    "        y=y_norm,\n",
    "        algorithm=svm.SVC,\n",
    "        args={'probability': False, 'kernel': 'rbf', 'C': 1}\n",
    "    ),\n",
    "    Test(\n",
    "        name='SVM poly',\n",
    "        X=X_norm,\n",
    "        y=y_norm,\n",
    "        algorithm=svm.SVC,\n",
    "        args={'probability': False, 'kernel': 'poly', 'C': 1}\n",
    "    )\n",
    "]"
   ]
  },
  {
   "source": [
    "### Run tests"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "\n",
      "SVM linear\n",
      "==========\n",
      "Training accuracy: 0.6485\n",
      "Test accuracy: 0.63\n",
      "\n",
      "\n",
      "SVM rbf\n",
      "==========\n",
      "Training accuracy: 0.647\n",
      "Test accuracy: 0.636\n",
      "\n",
      "\n",
      "SVM poly\n",
      "==========\n",
      "Training accuracy: 0.6415\n",
      "Test accuracy: 0.658\n",
      "\n",
      "\n",
      "SVM linear\n",
      "==========\n",
      "Training accuracy: 0.64\n",
      "Test accuracy: 0.664\n",
      "\n",
      "\n",
      "SVM rbf\n",
      "==========\n",
      "Training accuracy: 0.6495\n",
      "Test accuracy: 0.626\n",
      "\n",
      "\n",
      "SVM poly\n",
      "==========\n",
      "Training accuracy: 0.642\n",
      "Test accuracy: 0.656\n"
     ]
    }
   ],
   "source": [
    "for test in tests:\n",
    "    print(f'\\n\\n{test.name}\\n' + '='*10)\n",
    "    acc_train, acc_test = test.run()\n",
    "    print(f'Training accuracy: {acc_train}')\n",
    "    print(f'Test accuracy: {acc_test}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}