{
 "metadata": {
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python394jvsc74a57bd0daf19b0280700678d1b30283c052a0b4f397f6934ace6c653c7a54482fc82c11",
   "display_name": "Python 3.9.4 64-bit ('venv')"
  },
  "metadata": {
   "interpreter": {
    "hash": "9518128f597d7b00dc14729602cfd87fb7b2cf75925976bcb0d0e328a830a12b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Imports"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "source": [
    "# Data Cleaning"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Selecting Relevant Fields\n",
    "the dataset will be loaded and transformed and relevant dimensions will remain"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "  NAME_CONTRACT_STATUS CODE_GENDER FLAG_OWN_CAR FLAG_OWN_REALTY  CNT_CHILDREN  \\\n",
       "0             Approved           F            N               Y             0   \n",
       "1             Approved           F            Y               Y             1   \n",
       "2             Approved           F            N               Y             0   \n",
       "3             Approved           M            N               N             0   \n",
       "4              Refused           M            Y               N             0   \n",
       "5              Refused           M            Y               N             1   \n",
       "6             Approved           F            N               Y             1   \n",
       "7             Approved           M            N               N             0   \n",
       "8             Approved           F            N               N             0   \n",
       "9             Approved           M            N               Y             0   \n",
       "\n",
       "   AMT_INCOME_TOTAL  AMT_CREDIT  DAYS_BIRTH  DAYS_EMPLOYED  CNT_FAM_MEMBERS  \n",
       "0          171000.0    491580.0      -14548          -1187              2.0  \n",
       "1          175500.0     29700.0      -11081          -3244              3.0  \n",
       "2          135000.0     48600.0      -12939           -629              2.0  \n",
       "3          180000.0    196740.0       -8945           -672              2.0  \n",
       "4          225000.0    774229.5      -23919         365243              2.0  \n",
       "5          225000.0     36166.5      -15173          -3397              3.0  \n",
       "6           90000.0    120582.0      -18834         365243              3.0  \n",
       "7          135000.0     30550.5       -9950           -146              2.0  \n",
       "8           54000.0    112500.0      -23154         365243              1.0  \n",
       "9          315000.0     26811.0      -17154          -4006              2.0  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>NAME_CONTRACT_STATUS</th>\n      <th>CODE_GENDER</th>\n      <th>FLAG_OWN_CAR</th>\n      <th>FLAG_OWN_REALTY</th>\n      <th>CNT_CHILDREN</th>\n      <th>AMT_INCOME_TOTAL</th>\n      <th>AMT_CREDIT</th>\n      <th>DAYS_BIRTH</th>\n      <th>DAYS_EMPLOYED</th>\n      <th>CNT_FAM_MEMBERS</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Approved</td>\n      <td>F</td>\n      <td>N</td>\n      <td>Y</td>\n      <td>0</td>\n      <td>171000.0</td>\n      <td>491580.0</td>\n      <td>-14548</td>\n      <td>-1187</td>\n      <td>2.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Approved</td>\n      <td>F</td>\n      <td>Y</td>\n      <td>Y</td>\n      <td>1</td>\n      <td>175500.0</td>\n      <td>29700.0</td>\n      <td>-11081</td>\n      <td>-3244</td>\n      <td>3.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Approved</td>\n      <td>F</td>\n      <td>N</td>\n      <td>Y</td>\n      <td>0</td>\n      <td>135000.0</td>\n      <td>48600.0</td>\n      <td>-12939</td>\n      <td>-629</td>\n      <td>2.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Approved</td>\n      <td>M</td>\n      <td>N</td>\n      <td>N</td>\n      <td>0</td>\n      <td>180000.0</td>\n      <td>196740.0</td>\n      <td>-8945</td>\n      <td>-672</td>\n      <td>2.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Refused</td>\n      <td>M</td>\n      <td>Y</td>\n      <td>N</td>\n      <td>0</td>\n      <td>225000.0</td>\n      <td>774229.5</td>\n      <td>-23919</td>\n      <td>365243</td>\n      <td>2.0</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>Refused</td>\n      <td>M</td>\n      <td>Y</td>\n      <td>N</td>\n      <td>1</td>\n      <td>225000.0</td>\n      <td>36166.5</td>\n      <td>-15173</td>\n      <td>-3397</td>\n      <td>3.0</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>Approved</td>\n      <td>F</td>\n      <td>N</td>\n      <td>Y</td>\n      <td>1</td>\n      <td>90000.0</td>\n      <td>120582.0</td>\n      <td>-18834</td>\n      <td>365243</td>\n      <td>3.0</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>Approved</td>\n      <td>M</td>\n      <td>N</td>\n      <td>N</td>\n      <td>0</td>\n      <td>135000.0</td>\n      <td>30550.5</td>\n      <td>-9950</td>\n      <td>-146</td>\n      <td>2.0</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>Approved</td>\n      <td>F</td>\n      <td>N</td>\n      <td>N</td>\n      <td>0</td>\n      <td>54000.0</td>\n      <td>112500.0</td>\n      <td>-23154</td>\n      <td>365243</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>Approved</td>\n      <td>M</td>\n      <td>N</td>\n      <td>Y</td>\n      <td>0</td>\n      <td>315000.0</td>\n      <td>26811.0</td>\n      <td>-17154</td>\n      <td>-4006</td>\n      <td>2.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 149
    }
   ],
   "source": [
    "#TODO: remove the nrows argument when done testing\n",
    "df = pd.read_csv('database.csv', nrows=2500)[[\n",
    "    'NAME_CONTRACT_STATUS',\n",
    "    'CODE_GENDER',\n",
    "    'FLAG_OWN_CAR',\n",
    "    'FLAG_OWN_REALTY',\n",
    "    'CNT_CHILDREN',\n",
    "    'AMT_INCOME_TOTAL',\n",
    "    'AMT_CREDIT',\n",
    "    # 'NAME_INCOME_TYPE',\n",
    "    # 'NAME_EDUCATION_TYPE',\n",
    "    # 'NAME_FAMILY_STATUS',\n",
    "    # 'NAME_HOUSING_TYPE',\n",
    "    'DAYS_BIRTH',\n",
    "    'DAYS_EMPLOYED',\n",
    "    # 'OCCUPATION_TYPE',\n",
    "    'CNT_FAM_MEMBERS'\n",
    "]]\n",
    "\n",
    "# keep only approved and refused examples (then reset the pandas row indices - which does not happen automatically by default)\n",
    "df = df.loc[(df['NAME_CONTRACT_STATUS'] == 'Approved') | (df['NAME_CONTRACT_STATUS'] == 'Refused')].reset_index().drop(labels=['index'],axis=1)\n",
    "\n",
    "df.head(10)"
   ]
  },
  {
   "source": [
    "## Reductions to Binary Variables\n",
    "\n",
    "The target variable, NAME_CONTRACT_STATUS, will be reduced from one of 4 possible values, to one of two generic but still correct values â€“ for example, the dataset distingushes between cancelled and rejected and granted loans, however we will only distinguish between granted and not granted loans. Values that are binary but that don't use the binary alphabet will be transformed to use the binary alphabet as well.\n",
    "\n",
    "Defining a function that will return a copy of the dataframe with reduced fields."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce(df, name, value):\n",
    "    if type(name) != str:\n",
    "        raise Exception('only one dimension is reduced at a time')\n",
    "    idx_name = df.columns.get_loc(name)\n",
    "    reduced = [(1 if df[name][i] == value else 0) for i in range(len(df))]\n",
    "    df_reduced = df.drop(labels=[name], axis=1)\n",
    "    df_reduced.insert(loc=idx_name, column=name, value=reduced)\n",
    "    return df_reduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "df0 = reduce(df, 'NAME_CONTRACT_STATUS', 'Approved')  # 1 if approved else 0\n",
    "df1 = reduce(df0, 'CODE_GENDER', 'M')  # 1 if male else 0\n",
    "df2 = reduce(df1, 'FLAG_OWN_CAR', 'Y')  # 1 if owns car else 0\n",
    "df3 = reduce(df2, 'FLAG_OWN_REALTY', 'Y')  # 1 if owns property else 0"
   ]
  },
  {
   "source": [
    "## One-Hot Encoding\n",
    "defining a function that returns a copy of the input dataframe with a specific dimension one-hot encoded"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def one_hot_encode(df, name):\n",
    "    if type(name) != str:\n",
    "        raise Exception('one hot encoding applies to one dimension at a time')\n",
    "    if len(df) == 0:\n",
    "        raise Exception('dataframe is empty')\n",
    "    \n",
    "    df = df.copy()\n",
    "    values = df[name].unique()\n",
    "    \n",
    "    #for each unique value, we create a new column where df[row][new column] is 1 if the value of df[row][value] == new column\n",
    "    for v in values:\n",
    "        one_hot_column = [(1 if df[name][i] == v else 0) for i in range(len(df))]\n",
    "        df.insert(loc=len(df.loc[0]), column=v, value=one_hot_column)\n",
    "\n",
    "    return df.drop(labels=[name], axis=1)"
   ]
  },
  {
   "source": [
    "performing one-hot encoding on any dimension whose values are one of a set of string values"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   NAME_CONTRACT_STATUS  CODE_GENDER  FLAG_OWN_CAR  FLAG_OWN_REALTY  \\\n",
       "0                     1            0             0                1   \n",
       "1                     1            0             1                1   \n",
       "2                     1            0             0                1   \n",
       "3                     1            1             0                0   \n",
       "4                     0            1             1                0   \n",
       "5                     0            1             1                0   \n",
       "6                     1            0             0                1   \n",
       "7                     1            1             0                0   \n",
       "8                     1            0             0                0   \n",
       "9                     1            1             0                1   \n",
       "\n",
       "   CNT_CHILDREN  AMT_INCOME_TOTAL  AMT_CREDIT  DAYS_BIRTH  DAYS_EMPLOYED  \\\n",
       "0             0          171000.0    491580.0      -14548          -1187   \n",
       "1             1          175500.0     29700.0      -11081          -3244   \n",
       "2             0          135000.0     48600.0      -12939           -629   \n",
       "3             0          180000.0    196740.0       -8945           -672   \n",
       "4             0          225000.0    774229.5      -23919         365243   \n",
       "5             1          225000.0     36166.5      -15173          -3397   \n",
       "6             1           90000.0    120582.0      -18834         365243   \n",
       "7             0          135000.0     30550.5       -9950           -146   \n",
       "8             0           54000.0    112500.0      -23154         365243   \n",
       "9             0          315000.0     26811.0      -17154          -4006   \n",
       "\n",
       "   CNT_FAM_MEMBERS  \n",
       "0              2.0  \n",
       "1              3.0  \n",
       "2              2.0  \n",
       "3              2.0  \n",
       "4              2.0  \n",
       "5              3.0  \n",
       "6              3.0  \n",
       "7              2.0  \n",
       "8              1.0  \n",
       "9              2.0  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>NAME_CONTRACT_STATUS</th>\n      <th>CODE_GENDER</th>\n      <th>FLAG_OWN_CAR</th>\n      <th>FLAG_OWN_REALTY</th>\n      <th>CNT_CHILDREN</th>\n      <th>AMT_INCOME_TOTAL</th>\n      <th>AMT_CREDIT</th>\n      <th>DAYS_BIRTH</th>\n      <th>DAYS_EMPLOYED</th>\n      <th>CNT_FAM_MEMBERS</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>171000.0</td>\n      <td>491580.0</td>\n      <td>-14548</td>\n      <td>-1187</td>\n      <td>2.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>175500.0</td>\n      <td>29700.0</td>\n      <td>-11081</td>\n      <td>-3244</td>\n      <td>3.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>135000.0</td>\n      <td>48600.0</td>\n      <td>-12939</td>\n      <td>-629</td>\n      <td>2.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>180000.0</td>\n      <td>196740.0</td>\n      <td>-8945</td>\n      <td>-672</td>\n      <td>2.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>225000.0</td>\n      <td>774229.5</td>\n      <td>-23919</td>\n      <td>365243</td>\n      <td>2.0</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>225000.0</td>\n      <td>36166.5</td>\n      <td>-15173</td>\n      <td>-3397</td>\n      <td>3.0</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>90000.0</td>\n      <td>120582.0</td>\n      <td>-18834</td>\n      <td>365243</td>\n      <td>3.0</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>135000.0</td>\n      <td>30550.5</td>\n      <td>-9950</td>\n      <td>-146</td>\n      <td>2.0</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>54000.0</td>\n      <td>112500.0</td>\n      <td>-23154</td>\n      <td>365243</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>315000.0</td>\n      <td>26811.0</td>\n      <td>-17154</td>\n      <td>-4006</td>\n      <td>2.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 170
    }
   ],
   "source": [
    "# df4 = one_hot_encode(df3, 'NAME_INCOME_TYPE')\n",
    "# df5 = one_hot_encode(df4, 'NAME_EDUCATION_TYPE')\n",
    "# df6 = one_hot_encode(df5, 'NAME_FAMILY_STATUS')\n",
    "# df7 = one_hot_encode(df6, 'NAME_HOUSING_TYPE')\n",
    "# df8 = one_hot_encode(df7, 'OCCUPATION_TYPE')\n",
    "df8=df3\n",
    "\n",
    "#sanity check the dataframe before any work on it begins\n",
    "df8.head(10)"
   ]
  },
  {
   "source": [
    "# Training the models"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Split the dataframe into X and y as numpy arrays"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "convert the dataframe into a numpy tensor"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "409 0.5\n"
     ]
    }
   ],
   "source": [
    "data = np.array(df8)\n",
    "\n",
    "#undersample the target 1's\n",
    "num_zero = len(data[data[:,0]==0])\n",
    "num_one = num_zero\n",
    "\n",
    "#balance the amount of approvals and rejections\n",
    "data = np.vstack((\n",
    "    data[data[:,0]==1][:num_one,:],\n",
    "    data[data[:,0]==0]\n",
    "))\n",
    "np.random.shuffle(data)\n",
    "print(num_zero, num_one/float(len(data)))\n",
    "\n",
    "X = data[:,1:]\n",
    "y = data[:,0].reshape((-1,1)).astype(np.int32).ravel()"
   ]
  },
  {
   "source": [
    "normalize the data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "X = preprocessing.MinMaxScaler().fit_transform(X)"
   ]
  },
  {
   "source": [
    "## Creating the test class\n",
    "We will create a class that collects everything we need to build and to analyze a model. The class will point to our data, and to a model building algorithm, and will provide functionality for presenting results."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "class Test:\n",
    "    def __init__(self, name, X, y, algorithm, args):\n",
    "        self.name = name\n",
    "        self.X = X  #pointer, not copy\n",
    "        self.y = y\n",
    "        self.algorithm = algorithm\n",
    "        self.args = args\n",
    "        self.prediction=None\n",
    "        self.avg_acc_train=None\n",
    "        self.avg_acc_test=None\n",
    "    \n",
    "    def get_best_prediction(self):\n",
    "        if self.prediction is None:\n",
    "            raise Exception('the algorithm has not yet been run')\n",
    "        return self.prediction\n",
    "    \n",
    "    def get_avg_accuracies(self):\n",
    "        if self.avg_acc_train is None or self.avg_acc_test is None:\n",
    "            raise Exception('the algorithm has not yet been run')\n",
    "        return (self.avg_acc_train, self.avg_acc_test)\n",
    "    \n",
    "    def get_stats(self):\n",
    "        return (*self.get_avg_accuracies(), self.get_best_prediction())\n",
    "\n",
    "    #obtain accuracy using k-fold cross validation\n",
    "    def run(self, n_splits=5):\n",
    "        accs_train = []\n",
    "        accs_test = []\n",
    "        best_acc_test = 0\n",
    "        best_acc_index = -1\n",
    "        predictions = []\n",
    "        kfold_model = KFold(n_splits=n_splits, random_state=None, shuffle=False)\n",
    "        i = 0\n",
    "        for train_index, test_index in kfold_model.split(self.X):\n",
    "            X_train = self.X[train_index]\n",
    "            y_train = self.y[train_index]\n",
    "            X_test = self.X[test_index]\n",
    "            y_test = self.y[test_index]\n",
    "            model = self.algorithm(**self.args)\n",
    "            model.fit(X_train, y_train)\n",
    "            accs_train.append(model.score(X_train,y_train))\n",
    "            acc_test = model.score(X_test,y_test)\n",
    "            accs_test.append(acc_test)\n",
    "            if acc_test > best_acc_test:\n",
    "                best_acc_test = acc_test\n",
    "                best_acc_index = i\n",
    "            predictions.append(model.predict(X_test))\n",
    "            i += 1\n",
    "        self.avg_acc_train = sum(accs_train)/float(len(accs_train))\n",
    "        self.avg_acc_test = sum(accs_test)/float(len(accs_test))\n",
    "        self.prediction = predictions[best_acc_index]\n",
    "        return self.get_stats()\n",
    "    \n",
    "    def display(self):\n",
    "        print(f'\\n\\n{self.name}\\n' + '='*len(self.name))\n",
    "        print(f'Average training accuracy: {self.avg_acc_train}')\n",
    "        print(f'Average test accuracy: {self.avg_acc_test}')\n",
    "        print(f'Best prediction: {self.prediction}')"
   ]
  },
  {
   "source": [
    "## Support Vector Machine Modelling\n",
    "The motivation behind support vector machines is that we are building a line of best fit between two datasets, where \"best\" is defined by an objective function of distance between our line of best fit and between critical points, called support vectors, of these datasets. Support vectors are the closest points to a line of best fit. Our best fit line is also a decision boundary."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm"
   ]
  },
  {
   "source": [
    "### Tests comparing the three kernels"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "\n",
      "SVM linear\n",
      "==========\n",
      "Average training accuracy: 0.618884142213507\n",
      "Average test accuracy: 0.6014664073021099\n",
      "Best prediction: [1 0 1 0 1 0 1 1 0 0 1 1 1 0 0 1 1 1 1 1 1 1 1 0 1 1 1 0 1 0 1 1 1 0 1 0 1\n",
      " 0 0 0 1 1 1 0 0 0 1 0 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0\n",
      " 0 1 0 1 1 1 0 0 0 0 1 0 1 1 1 1 0 0 1 1 1 0 1 0 1 1 1 1 1 0 1 0 1 1 1 1 1\n",
      " 1 0 0 1 1 1 0 0 0 1 1 0 1 1 1 1 0 1 1 1 1 1 1 1 1 1 0 0 1 1 0 1 1 1 1 0 0\n",
      " 0 1 0 0 1 1 0 1 1 1 1 1 1 1 1]\n",
      "\n",
      "\n",
      "SVM rbf\n",
      "=======\n",
      "Average training accuracy: 0.7191264561010342\n",
      "Average test accuracy: 0.5818719138111627\n",
      "Best prediction: [1 0 0 0 0 0 1 1 0 0 1 1 0 0 0 1 1 0 0 0 0 1 0 0 0 1 1 1 1 1 1 0 1 0 1 0 1\n",
      " 0 1 1 0 1 1 0 0 0 1 1 0 1 0 1 1 0 0 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 0 1 1\n",
      " 1 0 0 1 0 1 0 0 0 1 1 0 0 1 0 0 0 0 1 1 0 1 1 1 1 1 0 1 0 0 1 1 1 0 0 1 0\n",
      " 1 1 1 1 1 0 0 0 1 1 0 0 1 1 0 0 1 1 1 1 0 0 1 1 0 0 1 0 0 1 0 1 1 1 0 1 0\n",
      " 1 0 0 1 1 0 1 0 1 1 1 1 0 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "svm_basic_tests = [\n",
    "    Test(\n",
    "        name='SVM linear',\n",
    "        X=X,\n",
    "        y=y,\n",
    "        algorithm=svm.SVC,\n",
    "        args={'kernel': 'linear', 'C': 100, 'class_weight': {0:1,1:1}}\n",
    "    ),\n",
    "    Test(\n",
    "        name='SVM rbf',\n",
    "        X=X,\n",
    "        y=y,\n",
    "        algorithm=svm.SVC,\n",
    "        args={'kernel': 'rbf', 'C': 100, 'class_weight': {0:1,1:1}}\n",
    "    ),\n",
    "]\n",
    "\n",
    "for test in svm_basic_tests:\n",
    "    test.run()\n",
    "    test.display()"
   ]
  },
  {
   "source": [
    "### Tests comparing different degrees of the polynomial kernel"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "\n",
      "SVM poly of degree 1\n",
      "====================\n",
      "Average training accuracy: 0.5000004668861031\n",
      "Average test accuracy: 0.5000074816699087\n",
      "Best prediction: [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "\n",
      "\n",
      "SVM poly of degree 2\n",
      "====================\n",
      "Average training accuracy: 0.5000004668861031\n",
      "Average test accuracy: 0.5000074816699087\n",
      "Best prediction: [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "\n",
      "\n",
      "SVM poly of degree 3\n",
      "====================\n",
      "Average training accuracy: 0.5000004668861031\n",
      "Average test accuracy: 0.5000074816699087\n",
      "Best prediction: [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "\n",
      "\n",
      "SVM poly of degree 4\n",
      "====================\n",
      "Average training accuracy: 0.501223241590214\n",
      "Average test accuracy: 0.49878796947478676\n",
      "Best prediction: [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "\n",
      "\n",
      "SVM poly of degree 5\n",
      "====================\n",
      "Average training accuracy: 0.5119214697574528\n",
      "Average test accuracy: 0.5000074816699087\n",
      "Best prediction: [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "\n",
      "\n",
      "SVM poly of degree 6\n",
      "====================\n",
      "Average training accuracy: 0.5235333940285267\n",
      "Average test accuracy: 0.5036734999251833\n",
      "Best prediction: [1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "\n",
      "\n",
      "SVM poly of degree 7\n",
      "====================\n",
      "Average training accuracy: 0.5388159768424493\n",
      "Average test accuracy: 0.5097785425707018\n",
      "Best prediction: [1 1 1 1 1 0 1 1 1 1 1 1 1 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "\n",
      "\n",
      "SVM poly of degree 8\n",
      "====================\n",
      "Average training accuracy: 0.5602082312019983\n",
      "Average test accuracy: 0.5158835852162202\n",
      "Best prediction: [1 1 1 1 0 0 1 1 1 1 1 1 0 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1\n",
      " 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 0\n",
      " 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "\n",
      "\n",
      "SVM poly of degree 9\n",
      "====================\n",
      "Average training accuracy: 0.5764101127529939\n",
      "Average test accuracy: 0.5293131827023791\n",
      "Best prediction: [1 1 1 1 0 0 1 1 1 1 1 1 0 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1\n",
      " 1 1 1 0 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 0 1 0 1 1 1 1 1 1 1 0\n",
      " 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "\n",
      "\n",
      "SVM poly of degree 10\n",
      "=====================\n",
      "Average training accuracy: 0.5981063099656839\n",
      "Average test accuracy: 0.523200658386952\n",
      "Best prediction: [1 1 1 1 0 0 1 1 1 1 1 1 0 0 1 1 1 1 1 1 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 0 1\n",
      " 1 1 1 0 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 0 1 0 1 1 1 1 0 1 1 0\n",
      " 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "\n",
      "\n",
      "SVM poly of degree 11\n",
      "=====================\n",
      "Average training accuracy: 0.614917944767374\n",
      "Average test accuracy: 0.5220035912015561\n",
      "Best prediction: [1 1 1 1 1 1 1 1 1 1 1 1 0 1 0 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1\n",
      " 1 0 1 1 1 1 1 1 1 1 1 0 1 1 1 0 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 0 0 1 1 0 1 0 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "\n",
      "\n",
      "SVM poly of degree 12\n",
      "=====================\n",
      "Average training accuracy: 0.6350878913089152\n",
      "Average test accuracy: 0.5171255424210683\n",
      "Best prediction: [1 0 0 0 1 1 1 1 1 1 1 1 0 1 0 1 0 1 0 1 1 1 1 1 0 1 1 1 1 1 1 0 0 1 1 1 1\n",
      " 1 0 1 1 1 1 1 1 1 0 1 0 1 1 1 0 1 1 1 0 1 1 1 1 1 0 1 1 1 1 1 0 1 1 1 1 1\n",
      " 1 0 0 1 1 0 1 0 1 1 0 0 1 1 1 0 0 1 1 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 0 1 1 1 1 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "\n",
      "\n",
      "SVM poly of degree 13\n",
      "=====================\n",
      "Average training accuracy: 0.6558671242150477\n",
      "Average test accuracy: 0.5220260362112824\n",
      "Best prediction: [1 0 0 0 1 1 1 1 1 1 1 1 0 1 0 1 0 1 0 1 1 1 1 1 0 1 1 1 1 1 1 0 0 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 0 0 1 0 1 1 1 0 1 1 1 0 1 1 1 1 1 0 1 1 1 1 1 0 1 1 0 1 1\n",
      " 1 0 1 1 1 0 1 1 1 1 0 0 1 1 1 0 0 1 1 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 0 1 1 1 1 1 1 0 1 1 1 1 1 0 1 1 1 1 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1\n",
      " 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "\n",
      "\n",
      "SVM poly of degree 14\n",
      "=====================\n",
      "Average training accuracy: 0.6690104349044051\n",
      "Average test accuracy: 0.5196019751608559\n",
      "Best prediction: [1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 0 1 1\n",
      " 1 1 1 1 1 1 1 0 1 1 0 1 1 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 0 1 1 1 1 1 1 0 1 0 1 0 1 0 1 1 1 1 1 1 0 0 1 1 1 1 1 1 1 0 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 0 0 1 1 1 0 1 1 0 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 0 1 1 1 1 1 0]\n",
      "\n",
      "\n",
      "SVM poly of degree 15\n",
      "=====================\n",
      "Average training accuracy: 0.6861241450148237\n",
      "Average test accuracy: 0.5245099506209786\n",
      "Best prediction: [1 1 1 0 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 0 1 1 0 1 1\n",
      " 1 1 1 1 1 1 1 0 1 1 0 1 1 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 0 1 1 1 1 1 0 0 1 0 1 0 1 0 1 1 1 1 1 1 0 0 1 1 1 1 1 1 1 0 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 0 0 1 1 1 0 1 1 0 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 0 1 1 1 1 1 0]\n",
      "\n",
      "\n",
      "SVM poly of degree 16\n",
      "=====================\n",
      "Average training accuracy: 0.6998800102714944\n",
      "Average test accuracy: 0.5244875056112525\n",
      "Best prediction: [1 1 1 0 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 0 1 1 0 1 1\n",
      " 1 1 1 1 1 1 1 0 1 1 0 1 1 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 0 1 1 1 1 1 1 0 1 0 1 0 1 0 1 1 1 1 0 1 0 0 1 1 0 1 1 1 1 0 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 0 0 0 1 1 1 0 1 1 0 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0]\n",
      "\n",
      "\n",
      "SVM poly of degree 17\n",
      "=====================\n",
      "Average training accuracy: 0.7108821812918737\n",
      "Average test accuracy: 0.5342660481819542\n",
      "Best prediction: [1 1 1 1 0 0 1 1 0 1 1 1 1 0 0 1 1 1 1 1 0 1 1 1 1 1 0 0 0 1 1 1 1 1 1 0 1\n",
      " 1 1 1 0 1 1 1 0 1 1 1 1 1 1 1 0 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 0 1 1 1 1 1 1 1 0\n",
      " 1 1 1 1 1 0 1 1 1 1 1 1 1 1 0 1 0 1 0 1 0 1 1 0 1 1 1 1 1 1 1 1 0 1 1 0 1\n",
      " 0 1 1 0 1 1 0 1 1 1 1 1 1 1 1 1]\n",
      "\n",
      "\n",
      "SVM poly of degree 18\n",
      "=====================\n",
      "Average training accuracy: 0.7221906295959101\n",
      "Average test accuracy: 0.5330240909771061\n",
      "Best prediction: [1 1 1 1 0 0 1 1 0 1 1 1 1 0 0 1 1 1 1 1 0 1 1 1 1 1 0 0 0 1 1 1 1 1 1 0 0\n",
      " 1 1 1 0 1 1 1 0 1 1 1 1 1 1 1 0 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 0 1 0 1 1 1 1 1 1 1 0\n",
      " 1 1 1 1 1 0 1 1 1 1 1 1 1 1 0 1 0 1 0 1 0 1 1 0 1 1 1 1 1 1 1 1 0 1 1 0 1\n",
      " 0 1 1 0 1 1 0 1 1 1 1 1 1 1 1 1]\n",
      "\n",
      "\n",
      "SVM poly of degree 19\n",
      "=====================\n",
      "Average training accuracy: 0.7331932675023928\n",
      "Average test accuracy: 0.5281610055364357\n",
      "Best prediction: [1 1 1 0 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 0 1 1 1 0 1 1 1 1 1 1 0 0 1 1 0 1 1\n",
      " 1 1 1 1 1 1 1 0 1 0 0 1 1 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 0 0 1 1 1 1 1 0 1 0 1 0 0 0 1 1 1 1 0 1 0 0 1 1 0 1 1 1 1 0 1 1 1 0 1 1\n",
      " 1 0 1 1 1 1 0 0 0 1 1 1 0 1 1 0 1 0 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0\n",
      " 1 1 1 0 1 1 1 1 1 1 1 1 1 1 0]\n",
      "\n",
      "\n",
      "SVM poly of degree 20\n",
      "=====================\n",
      "Average training accuracy: 0.7457235567383338\n",
      "Average test accuracy: 0.5306075115965884\n",
      "Best prediction: [1 1 1 0 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 0 0 1 1 1 0 1 1\n",
      " 1 0 1 1 1 1 0 0 1 0 0 1 1 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 0 1 1 1 1 1 1 0 1 0 1 0 0 0 1 1 1 1 0 1 0 0 1 1 0 1 1 1 0 1 1 1 1 0 1 1\n",
      " 1 0 1 0 1 0 1 0 0 1 1 1 0 1 1 0 1 0 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0\n",
      " 1 1 1 0 1 1 1 1 0 1 1 1 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "svm_poly_tests = [\n",
    "    Test(\n",
    "        name=f'SVM poly of degree {i}',\n",
    "        X=X,\n",
    "        y=y,\n",
    "        algorithm=svm.SVC,\n",
    "        args={'class_weight': {0:1,1:2}, 'kernel': 'poly', 'C': 0.01, 'degree': i}\n",
    "    )\n",
    "    for i in range(1,21)\n",
    "]\n",
    "\n",
    "for test in svm_poly_tests:\n",
    "    test.run()\n",
    "    test.display()"
   ]
  },
  {
   "source": [
    "## Logistic Regression Modelling"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore', category=ConvergenceWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n\nLogreg l1 penalty with âˆ†=0.0001\n===============================\nAverage training accuracy: 0.4990825688073395\nAverage test accuracy: 0.5036585365853659\nBest prediction: [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n\n\nLogreg l1 penalty with âˆ†=0.001\n==============================\nAverage training accuracy: 0.5042818124518524\nAverage test accuracy: 0.48293431093820144\nBest prediction: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n\n\nLogreg l1 penalty with âˆ†=0.01\n=============================\nAverage training accuracy: 0.5045871559633028\nAverage test accuracy: 0.48170731707317077\nBest prediction: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n\n\nLogreg l1 penalty with âˆ†=0.1\n============================\nAverage training accuracy: 0.5266050377010528\nAverage test accuracy: 0.4914559329642376\nBest prediction: [1 1 1 0 1 1 1 0 0 0 1 0 0 1 1 1 1 0 1 1 0 0 1 1 1 1 0 0 1 1 1 1 1 1 0 1 0\n 1 1 0 0 1 1 1 1 1 0 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 1 1 1 1 1 1 0 0 1 0 0\n 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 0 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 0 1\n 1 1 1 1 0 1 1 0 1 1 1 1 0 1 1 1 1 0 1 0 1 0 1 0 1 0 1 1 1 0 1 1 1 1 1 1 1\n 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1]\n\n\nLogreg l1 penalty with âˆ†=1\n==========================\nAverage training accuracy: 0.6344739360832925\nAverage test accuracy: 0.6234475534939398\nBest prediction: [1 0 1 0 0 0 0 0 0 0 1 1 1 0 0 1 1 1 1 1 1 1 0 0 1 1 1 0 1 0 1 1 1 0 1 0 1\n 0 0 0 1 1 1 0 0 0 1 0 1 1 0 1 1 1 0 0 1 1 1 1 1 1 1 1 1 0 0 1 1 0 1 0 1 0\n 0 0 0 1 1 1 0 0 0 0 1 0 1 1 1 1 0 0 1 1 1 0 1 0 1 0 1 1 1 0 1 0 1 1 0 0 1\n 1 0 0 1 1 0 0 0 0 0 1 0 1 1 1 0 0 0 1 1 0 1 1 1 0 0 0 0 1 1 0 1 0 1 1 0 0\n 0 1 0 1 1 0 0 1 1 1 1 1 1 1 1]\n\n\nLogreg l1 penalty with âˆ†=10\n===========================\nAverage training accuracy: 0.6323360646170367\nAverage test accuracy: 0.621008529103696\nBest prediction: [1 0 1 0 0 0 1 0 0 0 1 1 0 0 0 1 1 1 0 1 1 1 1 0 1 1 1 0 1 0 1 0 1 0 1 0 1\n 0 0 0 1 0 1 0 0 0 1 0 1 1 1 1 1 1 0 0 1 1 1 1 1 1 1 1 1 0 1 1 1 0 1 0 1 0\n 0 0 0 1 1 1 0 0 0 0 1 0 1 0 1 1 0 0 1 1 1 0 1 0 1 1 1 1 1 0 1 0 1 1 0 0 1\n 1 0 0 1 1 0 0 0 0 0 1 0 1 1 1 1 0 0 1 1 0 1 1 0 0 1 0 0 1 1 0 1 0 1 1 0 0\n 0 1 0 1 1 0 0 1 0 1 1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "logreg_l1_tests = [\n",
    "    Test(\n",
    "        name=f'Logreg l1 penalty with âˆ†={i}',\n",
    "        X=X,\n",
    "        y=y,\n",
    "        algorithm=linear_model.LogisticRegression,\n",
    "        args={'class_weight': {0:1,1:1}, 'penalty':'l1', 'solver':'saga', 'C':i}\n",
    "    )\n",
    "    for i in [0.0001, 0.001, 0.01, 0.1, 1, 10]\n",
    "]\n",
    "\n",
    "for test in logreg_l1_tests:\n",
    "    test.run()\n",
    "    test.display()"
   ]
  },
  {
   "source": [],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "\n",
      "Logreg l2 penalty with âˆ†=0.0001\n",
      "===============================\n",
      "Average training accuracy: 0.5000004668861031\n",
      "Average test accuracy: 0.5000074816699087\n",
      "Best prediction: [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "\n",
      "\n",
      "Logreg l2 penalty with âˆ†=0.001\n",
      "==============================\n",
      "Average training accuracy: 0.5000004668861031\n",
      "Average test accuracy: 0.5000074816699087\n",
      "Best prediction: [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "\n",
      "\n",
      "Logreg l2 penalty with âˆ†=0.01\n",
      "=============================\n",
      "Average training accuracy: 0.5000004668861031\n",
      "Average test accuracy: 0.5000074816699087\n",
      "Best prediction: [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "\n",
      "\n",
      "Logreg l2 penalty with âˆ†=0.1\n",
      "============================\n",
      "Average training accuracy: 0.503668790998436\n",
      "Average test accuracy: 0.5012344755349394\n",
      "Best prediction: [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "\n",
      "\n",
      "Logreg l2 penalty with âˆ†=1\n",
      "==========================\n",
      "Average training accuracy: 0.5443186964540001\n",
      "Average test accuracy: 0.5403785724973814\n",
      "Best prediction: [1 1 0 0 1 1 1 1 1 1 1 1 1 0 1 1 1 0 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 0\n",
      " 1 1 1 0 1 0 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "\n",
      "\n",
      "Logreg l2 penalty with âˆ†=10\n",
      "===========================\n",
      "Average training accuracy: 0.5644923780843664\n",
      "Average test accuracy: 0.5574816699087236\n",
      "Best prediction: [1 1 0 0 0 1 1 1 1 1 1 1 1 0 1 1 1 0 1 1 0 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1\n",
      " 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 0\n",
      " 1 1 1 0 1 0 1 1 1 1 1 0 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1]\n",
      "\n",
      "\n",
      "Logreg l2 penalty with âˆ†=100\n",
      "============================\n",
      "Average training accuracy: 0.5660214300721339\n",
      "Average test accuracy: 0.5623597186892114\n",
      "Best prediction: [1 1 0 0 0 1 1 1 1 1 1 1 1 0 1 1 1 0 1 1 0 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1\n",
      " 0 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 0 0 1 1 1 1 1 1 0\n",
      " 1 1 1 0 1 0 1 1 1 1 1 0 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 0 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1]\n",
      "\n",
      "\n",
      "Logreg l2 penalty with âˆ†=1000\n",
      "=============================\n",
      "Average training accuracy: 0.5660214300721339\n",
      "Average test accuracy: 0.5635867125542421\n",
      "Best prediction: [1 1 0 0 0 1 1 1 1 1 1 1 1 0 1 1 1 0 1 1 0 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1\n",
      " 0 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 0 0 1 1 1 1 1 1 0\n",
      " 1 1 1 0 1 0 1 1 1 1 1 0 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 0 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "logreg_l2_tests = [\n",
    "    Test(\n",
    "        name=f'Logreg l2 penalty with âˆ†={i}',\n",
    "        X=X,\n",
    "        y=y,\n",
    "        algorithm=linear_model.LogisticRegression,\n",
    "        args={'class_weight': {1:2}, 'penalty':'l2', 'solver':'saga', 'C':i}\n",
    "    )\n",
    "    for i in [0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
    "]\n",
    "\n",
    "for test in logreg_l2_tests:\n",
    "    test.run()\n",
    "    test.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}